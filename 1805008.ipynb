{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "62o7KnWioYy6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Input:\n",
        "\t# input shape: (batch_size, #features)\n",
        "\t# output shape: (batch_size, #features)\n",
        "\n",
        "\tdef __init__(self, inputFeatures) -> None:\n",
        "\t\tself.inputShape = (-1, inputFeatures)\n",
        "\t\tself.outputShape = self.inputShape\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\treturn x\n",
        "\n",
        "\tdef backward(self, gradientLossWRTOutput, _):\n",
        "\t\treturn gradientLossWRTOutput, None\n",
        "\n",
        "\tdef getWeights(self):\n",
        "\t\treturn None\n",
        "\n",
        "\n",
        "\n",
        "class Dense:\n",
        "\t# input shape: (batch_size, #features)\n",
        "\t# output shape: (batch_size, #nodes)\n",
        "\n",
        "\tdef __init__(self, numNodes) -> None:\n",
        "\t\tself.numNodes = numNodes\n",
        "\n",
        "\t# input shape: (batch_size, #features)\n",
        "\tdef initPipeline(self, inputShape):\n",
        "\t\tinputFeatures = inputShape[1]\n",
        "\t\tself.features = inputFeatures\n",
        "\n",
        "\t\t# self.weights = np.random.randn(self.numNodes, inputFeatures)\n",
        "\t\tself.weights = np.random.uniform(-.3, .3, (self.numNodes, inputFeatures))\n",
        "\n",
        "\t\t# self.weights = np.ones((self.numNodes, inputFeatures))\n",
        "\t\t# print(self.weights.shape)\n",
        "\t\t# self.bias = np.random.randn(self.numNodes, 1)\n",
        "\t\tself.bias = np.random.uniform(-.3, .3, (self.numNodes, 1))\n",
        "\n",
        "\t\t# self.bias = np.ones((self.numNodes, 1))\n",
        "\t\t# print(self.bias.shape)\n",
        "\t\tself.outputShape = (-1, self.numNodes)\n",
        "\n",
        "\t# x shape: (batch_size, #features)\n",
        "\tdef forward(self, x):\n",
        "\t\tself.x = x\n",
        "\t\tself.y = np.dot(self.weights, x.T) + self.bias\n",
        "\t\tself.y = self.y.T\n",
        "\t\treturn self.y\n",
        "\n",
        "\t# gradientLossWRTOutput shape: (batch_size, #nodes)\n",
        "\n",
        "\tdef backward(self, gradientLossWRTOutput,optimizer):\n",
        "\n",
        "\t\tgradientLossWRTInput = np.dot(gradientLossWRTOutput, self.weights)\n",
        "\n",
        "\n",
        "\t\tgradientLossWRTWeights = np.dot(gradientLossWRTOutput.T, self.x)\n",
        "\t\tgradientLossWRTBias = np.sum(gradientLossWRTOutput, axis = 0, keepdims=True).T\n",
        "\n",
        "\t\t# print(f\"gradientLossWRTWeights : {gradientLossWRTWeights.shape}\")\n",
        "\t\t# print(f\"gradientLossWRTbias : {gradientLossWRTBias.shape}\")\n",
        "\n",
        "\n",
        "\t\tself.weights = optimizer.update(self.weights, gradientLossWRTWeights)\n",
        "\t\tself.bias = optimizer.update(self.bias, gradientLossWRTBias)\n",
        "\n",
        "\t\t# self.weights -= learningRate * gradientLossWRTWeights\n",
        "\t\t# self.bias -= learningRate * gradientLossWRTBias\n",
        "\n",
        "\t\treturn gradientLossWRTInput, (gradientLossWRTWeights, gradientLossWRTBias)\n",
        "\n",
        "\tdef getWeights(self):\n",
        "\t\treturn (self.weights, self.bias)\n",
        "\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "\tdef __init__(self) -> None:\n",
        "\t\tpass\n",
        "\n",
        "\t# input shape: (batch_size, #features)\n",
        "\t# output shape: (batch_size, #features)\n",
        "\n",
        "\tdef initPipeline(self, inputShape):\n",
        "\t\tself.inputShape = inputShape\n",
        "\t\tself.outputShape = inputShape\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tself.x = x\n",
        "\t\t# print(self.x)\n",
        "\t\t# print(np.max(self.x))\n",
        "\t\tself.y =  np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "\t\treturn self.y\n",
        "\n",
        "\t# gradientLossWRTOutput shape: (batch_size, #features)\n",
        "\n",
        "\tdef backward(self, gradientLossWRTOutput, _):\n",
        "\t\t# gradientOutputWRT\n",
        "\n",
        "\t\tn , m = self.y.shape\n",
        "\n",
        "\t\tgradientOutputWRTInput = np.repeat(self.y, m, axis=0).reshape(n, m, m)\n",
        "\t\tgradientOutputWRTInput = np.multiply(gradientOutputWRTInput, np.transpose(gradientOutputWRTInput, axes=(0, 2, 1))) * -1\n",
        "\n",
        "\t\tdiagElems = np.reshape(self.y, (n,  m , 1))\n",
        "\t\tdiagElems = diagElems * (1 - diagElems)\n",
        "\t\tdiagElems = np.eye(m) * diagElems\n",
        "\t\tmask = np.eye(m, dtype=bool)\n",
        "\t\tmask = np.tile(mask, (n, 1)).reshape(n, m, m)\n",
        "\t\tgradientOutputWRTInput[mask] = 0\n",
        "\t\tgradientOutputWRTInput = gradientOutputWRTInput + diagElems\n",
        "\n",
        "\t\t# jacobian_matrix = np.zeros((n, m, m))\n",
        "\n",
        "\t\t# for i in range(n):\n",
        "\t\t# \t# s = np.exp(self.x[i]) / np.sum(np.exp(self.x[i]), axis=1, keepdims=True)\n",
        "\t\t# \ts = self.y[i]\n",
        "\t\t# \tfor j in range(n):\n",
        "\t\t# \t\tfor k in range(n):\n",
        "\t\t# \t\t\tjacobian_matrix[i, j, k] = s[j] * (int(j == k) - s[k])\n",
        "\n",
        "\n",
        "\t\t# print(np.isclose(jacobian_matrix, gradientOutputWRTInput).all())\n",
        "\t\t# print(f\"gradientLossWRTOutput : {np.expand_dims(gradientLossWRTOutput, -1).shape}\")\n",
        "\n",
        "\n",
        "\t\tgradientLossWRTInput = np.matmul(gradientOutputWRTInput, np.expand_dims(gradientLossWRTOutput, -1))\n",
        "\t\tgradientLossWRTInput = np.squeeze(gradientLossWRTInput)\n",
        "\n",
        "\t\t# print(f\"gradientLossWRTInput : {gradientLossWRTInput.shape}\")\n",
        "\n",
        "\t\treturn gradientLossWRTInput, None\n",
        "\n",
        "\tdef getWeights(self):\n",
        "\t\treturn None\n",
        "\n",
        "\n",
        "class Relu:\n",
        "\n",
        "\tdef __init__(self) -> None:\n",
        "\t\tpass\n",
        "\n",
        "\tdef initPipeline(self, inputShape):\n",
        "\t\tself.inputShape = inputShape\n",
        "\t\tself.outputShape = inputShape\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tself.x = x\n",
        "\t\tself.y = np.maximum(x, 0)\n",
        "\t\treturn self.y\n",
        "\n",
        "\t# gradientLossWRTOutput shape: (batch_size, #features)\n",
        "\tdef backward(self, gradientLossWRTOutput, _):\n",
        "\t\tgradientOutputWRTInput = np.where(self.x > 0, 1, 0)\n",
        "\t\tgradientLossWRTInput = np.multiply(gradientOutputWRTInput, gradientLossWRTOutput)\n",
        "\t\treturn gradientLossWRTInput, None\n",
        "\n",
        "\tdef getWeights(self):\n",
        "\t\treturn None\n",
        "\n",
        "\n",
        "\n",
        "class Model:\n",
        "\tEPSILON = .000001\n",
        "\tdef __init__(self, *layers) -> None:\n",
        "\t\tself.nLayers = len(layers)\n",
        "\n",
        "\t\tfor i in range(1, self.nLayers):\n",
        "\t\t\tlayers[i].initPipeline(layers[i-1].outputShape)\n",
        "\n",
        "\t\tself.layers = layers\n",
        "\n",
        "\tdef predict(self, x):\n",
        "\t\tfor layer in self.layers:\n",
        "\t\t\tx = layer.forward(x)\n",
        "\t\treturn x\n",
        "\n",
        "\tdef crossEntropyGradient(self, yTrue, yPred):\n",
        "\t\typc = yPred.copy()\n",
        "\t\typc[ypc < Model.EPSILON] = Model.EPSILON\n",
        "\t\treturn - yTrue / ypc\n",
        "\n",
        "\tdef __backprop(self,yTrue, yPred, optimizer):\n",
        "\t\tgradientLossWRTOutput = self.crossEntropyGradient(yTrue, yPred)\n",
        "\t\t# gradientLossWRTOutput = self.squaredErrorGradient(yTrue, yPred)\n",
        "\t\t# print(gradientLossWRTOutput)\n",
        "\n",
        "\t\t# print(f\"loss grad wrt yhat : {gradientLossWRTOutput.shape}\")\n",
        "\t\t# wAndg = []\n",
        "\t\tfor layer in reversed(self.layers):\n",
        "\t\t\tgradientLossWRTOutput, g = layer.backward(gradientLossWRTOutput, optimizer)\n",
        "\t\t\t# wAndg.append((layer.getWeights(), g))\n",
        "\n",
        "\n",
        "\tdef crossEntropyLoss(self, yPred, yTrue):\n",
        "\t\typc = yPred.copy()\n",
        "\t\typc[ypc < Model.EPSILON] = Model.EPSILON\n",
        "\t\treturn -np.sum(yTrue * np.log(ypc))\n",
        "\n",
        "\n",
        "\t\t# return wAndg\n",
        "\n",
        "\tdef train(self, x, y, optimizer, epoch, batchSize):\n",
        "\t\tfor i in range(epoch):\n",
        "\t\t\tnumExa = x.shape[0]\n",
        "\n",
        "\t\t\txy = list(zip(x, y))\n",
        "\t\t\tnp.random.shuffle(xy)\n",
        "\n",
        "\t\t\tx, y = zip(*xy)\n",
        "\t\t\tx = np.array(x)\n",
        "\t\t\ty = np.array(y)\n",
        "\n",
        "\t\t\tnumExa = x.shape[0]\n",
        "\t\t\tnumBatches = numExa // batchSize\n",
        "\t\t\tremSamples = numExa % batchSize\n",
        "\n",
        "\t\t\txB = np.array_split(x[:numBatches * batchSize], numBatches)\n",
        "\t\t\tyB = np.array_split(y[:numBatches * batchSize], numBatches)\n",
        "\n",
        "\t\t\tif remSamples > 0:\n",
        "\t\t\t\txB.append(x[-remSamples:])\n",
        "\t\t\t\tyB.append(y[-remSamples:])\n",
        "\t\t\t\tnumBatches += 1\n",
        "\n",
        "\t\t\tfor i in range(numBatches):\n",
        "\t\t\t\txc = xB[i]\n",
        "\t\t\t\tyc = yB[i]\n",
        "\n",
        "\t\t\t\tyPred = self.predict(xc)\n",
        "\t\t\t\t# print(yPred)\n",
        "\t\t\t\t# loss = self.__loss(y, yPred)\n",
        "\t\t\t\t# print(f\"epoch {i} : {loss}\")\n",
        "\t\t\t\tself.__backprop(yc, yPred, optimizer)\n",
        "\n",
        "\t\t\t\tprint(f\"loss : {self.crossEntropyLoss(yc, yPred)}\")\n",
        "\n",
        "\n",
        "\tdef squaredErrorGradient(self, yTrue, yPred):\n",
        "\t\treturn -2 * (yTrue - yPred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GradientDescent:\n",
        "\tdef __init__(self, learningRate) -> None:\n",
        "\t\tself.learningRate = learningRate\n",
        "\n",
        "\tdef update(self, w, g):\n",
        "\t\treturn w - self.learningRate * g\n",
        "\n",
        "\n",
        "def eqn(x):\n",
        "\treturn 2 * (x[0] ** 2) + 3.5 * x[1] + 7\n",
        "\n",
        "def main():\n",
        "\n",
        "\txs = []\n",
        "\tys = []\n",
        "\tfor i in range(100):\n",
        "\t\tx = np.random.randn(2)\n",
        "\t\txs.append(x)\n",
        "\t\tys.append(eqn(x))\n",
        "\tx = np.vstack(xs)\n",
        "\ty = np.vstack(ys)\n",
        "\n",
        "\tprint(x.shape)\n",
        "\tprint(y.shape)\n",
        "\n",
        "\n",
        "\n",
        "\tmodel = Model(\n",
        "\t\tInput(2),\n",
        "\t\tDense(4),\n",
        "\t\tRelu(),\n",
        "\t\tDense(3),\n",
        "\t\tRelu(),\n",
        "\t\tDense(1)\n",
        "\t\t# Dense(3),\n",
        "\t\t# Dense(2),\n",
        "\t\t# Softmax()\n",
        "\t\t# Softmax()\n",
        "\t)\n",
        "\t# print(model.predict(x))\n",
        "\t# x = np.array([[1, -3], [-1, 1], [5, 6]])\n",
        "\t# y = np.array([[0, 1, 0], [1, 0, 0], [0,0, 1]])\n",
        "\t# print(x.shape)\n",
        "\t# yPred = model.predict(x)\n",
        "\t# print(f\"ypred : {yPred}\")\n",
        "\t# print(f\"yPred.shape : {yPred.shape}\")\n",
        "\txt = np.array([[.5, .36]])\n",
        "\n",
        "\t# print(model.predict(xt))\n",
        "\t# print(eqn(xt[0]))\n",
        "\n",
        "\tprint(np.sum(np.square(model.predict(x) - y)) / 100)\n",
        "\n",
        "\tmodel.train(x, y, GradientDescent(0.0001), 1000)\n",
        "\n",
        "\tprint(np.sum(np.square(model.predict(x) - y)) / 100)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tprint(model.predict(xt))\n",
        "\tprint(eqn(xt[0]))\n",
        "\t# print(model.predict(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as ds\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "k2ZScuBHopbw"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = ds.EMNIST(root = '/content/', split = 'letters', train = True, transform = transforms.ToTensor(), download = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ixQkIqmoxoc",
        "outputId": "b17600a2-4c93-41fa-dfc9-48ffea4aeb6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to /content/EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 561753746/561753746 [00:10<00:00, 51450151.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/EMNIST/raw/gzip.zip to /content/EMNIST/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu2kTH6_o-dl",
        "outputId": "68bfd36d-a972-40a2-b3e0-c9f7b53bc100"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset EMNIST\n",
              "    Number of datapoints: 124800\n",
              "    Root location: /content/\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train.targets == 26).any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNsXUAVzrrj6",
        "outputId": "d2f926ec-110b-4c7f-cfcd-708f0b6d0a5e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8px6fpppKJE",
        "outputId": "d99321fb-7e17-4f9b-cb8b-e6235f3a9f6f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([124800, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.data[0]"
      ],
      "metadata": {
        "id": "StZ3GpaXpTDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder(sparse=False)\n",
        "oneHotEncoded = encoder.fit_transform(np.expand_dims(train.targets, -1))\n",
        "# print(one_hot_encoded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shuFhun7y8AS",
        "outputId": "d92a5dcb-c4fd-495a-b398-5e123c491578"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tX, vX, tY, vY = train_test_split(train.data, oneHotEncoded, test_size = .6, random_state = 29)"
      ],
      "metadata": {
        "id": "ZHEU-jg_paSG"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tX.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjZP1qXVr5Vt",
        "outputId": "864d110b-aebd-46d5-ce41-b727fc1301b6"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([49920, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tY.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMxpiK4oyvFn",
        "outputId": "d792d107-ad22-477d-9a0d-a41a1bd2f5b9"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49920, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(img):\n",
        "  plt.imshow(img, cmap = 'gray')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "JKIqRod-r6oL"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def randomPlot(x, y):\n",
        "  num_images = x.shape[0]\n",
        "  indices = np.random.choice(num_images, 16, replace=False)\n",
        "\n",
        "  # Plotting 16 random images\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  for i, index in enumerate(indices, 1):\n",
        "      plt.subplot(4, 4, i)\n",
        "      plt.imshow(x[index], cmap='gray')\n",
        "      plt.title(f\"{chr(np.argmax(y[index]) + ord('a') )}\", fontsize=10)\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "j41qxAFRtYuT"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomPlot(tX, tY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "id": "7QwGB5wAtloa",
        "outputId": "03a051e4-7855-47f4-90c6-3d3da402bb80"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def acc(model, x, yTrue):\n",
        "  yPred = model.predict(x)\n",
        "  yPred = np.argmax(yPred, axis = 1)\n",
        "  yTrue = np.argmax(yTrue, axis = 1)\n",
        "  return accuracy_score(yTrue, yPred)"
      ],
      "metadata": {
        "id": "qA0jNIAbHKHb"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(13)\n",
        "x = np.copy(tX)\n",
        "y = np.copy(tY)\n",
        "\n",
        "numExamples = x.shape[0]\n",
        "x = np.reshape(x, (numExamples, -1))\n",
        "print(x.shape)\n",
        "numFeatures = x.shape[1]\n",
        "\n",
        "model = Model(\n",
        "    Input(numFeatures),\n",
        "    Dense(256),\n",
        "    Relu(),\n",
        "    Dense(128),\n",
        "    Relu(),\n",
        "    Dense(64),\n",
        "    Relu(),\n",
        "    Dense(32),\n",
        "    Relu(),\n",
        "    Dense(26),\n",
        "    Softmax()\n",
        ")\n",
        "\n",
        "\n",
        "n = x.shape[0]\n",
        "xs = x[0:n] / 255\n",
        "ys = y[0:n]\n",
        "\n",
        "print(acc(model, xs, ys))\n",
        "\n",
        "model.train(xs,ys, GradientDescent(.001), 10, 256)\n",
        "\n",
        "print(acc(model, xs, ys))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gBeBHqbx2kK",
        "outputId": "1b4a5e06-aba3-4167-f8ba-b9630658de28"
      },
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(49920, 784)\n",
            "0.03467548076923077\n",
            "loss : 3419.031401347758\n",
            "loss : 3420.2485584043993\n",
            "loss : 3407.576298041206\n",
            "loss : 3395.7027579551195\n",
            "loss : 3396.204833644987\n",
            "loss : 3399.9808198697438\n",
            "loss : 3393.3117222745364\n",
            "loss : 3400.742251747944\n",
            "loss : 3399.5128278379902\n",
            "loss : 3400.5679941865897\n",
            "loss : 3396.3948162468114\n",
            "loss : 3396.0041389125786\n",
            "loss : 3396.9712744761227\n",
            "loss : 3396.5428476663155\n",
            "loss : 3395.9634561767593\n",
            "loss : 3409.124288159034\n",
            "loss : 3443.1734800862073\n",
            "loss : 3400.222905513132\n",
            "loss : 3399.606420035134\n",
            "loss : 3396.2134537876263\n",
            "loss : 3398.6286415256973\n",
            "loss : 3400.119342862719\n",
            "loss : 3402.318678476288\n",
            "loss : 3400.641713300508\n",
            "loss : 3400.3982801627335\n",
            "loss : 3393.911879618974\n",
            "loss : 3399.636798275131\n",
            "loss : 3397.299380044395\n",
            "loss : 3394.504335784446\n",
            "loss : 3398.680600948853\n",
            "loss : 3391.7404174979156\n",
            "loss : 3392.77607925003\n",
            "loss : 3395.5732272265823\n",
            "loss : 3376.2874552181065\n",
            "loss : 3392.9798276828005\n",
            "loss : 3389.158431438288\n",
            "loss : 3381.7758545687334\n",
            "loss : 3398.807040783558\n",
            "loss : 3391.115736169882\n",
            "loss : 3366.7051773097533\n",
            "loss : 3377.5504916231184\n",
            "loss : 3341.6639170427397\n",
            "loss : 3388.7678720053746\n",
            "loss : 3377.781604702214\n",
            "loss : 3346.428535901531\n",
            "loss : 3329.8701288216635\n",
            "loss : 3343.743061280884\n",
            "loss : 3372.296082957119\n",
            "loss : 3322.445093610002\n",
            "loss : 3255.2995875494967\n",
            "loss : 3353.4702851767233\n",
            "loss : 3331.928480698816\n",
            "loss : 3262.546917751568\n",
            "loss : 3271.8250629221766\n",
            "loss : 3258.9572978641036\n",
            "loss : 3294.0648812750355\n",
            "loss : 3309.500672850763\n",
            "loss : 3264.7947753519757\n",
            "loss : 3346.1080746954885\n",
            "loss : 3270.2034730747505\n",
            "loss : 3268.801531081017\n",
            "loss : 3329.87040990858\n",
            "loss : 3251.4035876731414\n",
            "loss : 3231.540907379744\n",
            "loss : 3306.1348974417615\n",
            "loss : 3325.7297369823864\n",
            "loss : 3272.8785194729526\n",
            "loss : 3171.8847190401425\n",
            "loss : 3306.0061215151136\n",
            "loss : 3284.6408132354754\n",
            "loss : 3236.0703519015283\n",
            "loss : 3208.241512832446\n",
            "loss : 3171.722043186249\n",
            "loss : 3213.739953537456\n",
            "loss : 3307.0121517956413\n",
            "loss : 3278.171800059753\n",
            "loss : 3244.7942236036633\n",
            "loss : 3267.0794546183697\n",
            "loss : 3248.0566829669033\n",
            "loss : 3213.8435875195805\n",
            "loss : 3255.645546223115\n",
            "loss : 3234.807661565882\n",
            "loss : 3192.853178654026\n",
            "loss : 3282.701370052977\n",
            "loss : 3210.0948170457395\n",
            "loss : 3199.7281879679545\n",
            "loss : 3173.7005000340896\n",
            "loss : 3258.0157957917245\n",
            "loss : 3217.731419370927\n",
            "loss : 3206.219994817964\n",
            "loss : 3191.7884099314556\n",
            "loss : 3260.463277541199\n",
            "loss : 3239.7958284878537\n",
            "loss : 3085.0943371000158\n",
            "loss : 3130.609828084698\n",
            "loss : 3170.642534514379\n",
            "loss : 3137.1462759916526\n",
            "loss : 3204.573614212731\n",
            "loss : 3206.317423578157\n",
            "loss : 3082.5382147202045\n",
            "loss : 3190.2719137745075\n",
            "loss : 3101.2471839948194\n",
            "loss : 3168.170305883683\n",
            "loss : 3165.993024390381\n",
            "loss : 3106.700705834448\n",
            "loss : 3142.8328330571435\n",
            "loss : 3131.343030326898\n",
            "loss : 3163.8808840344\n",
            "loss : 3143.5157283512535\n",
            "loss : 3213.4734580972136\n",
            "loss : 3082.086990648366\n",
            "loss : 3136.5448951000776\n",
            "loss : 3092.652149000396\n",
            "loss : 3126.157724029693\n",
            "loss : 3147.390879780819\n",
            "loss : 3023.820803145646\n",
            "loss : 3152.116824633971\n",
            "loss : 3085.8976788028576\n",
            "loss : 3108.442440422903\n",
            "loss : 3104.2070731321637\n",
            "loss : 3043.7156167127014\n",
            "loss : 3090.2052109172937\n",
            "loss : 3127.042103065317\n",
            "loss : 3030.978300209482\n",
            "loss : 3178.8040932495933\n",
            "loss : 3131.318094496028\n",
            "loss : 3024.5793596779495\n",
            "loss : 3051.426285143378\n",
            "loss : 3011.137981528087\n",
            "loss : 3037.342369412311\n",
            "loss : 3003.9368691233803\n",
            "loss : 2906.1775770341023\n",
            "loss : 2912.4061784295254\n",
            "loss : 3084.64430355114\n",
            "loss : 3033.2831636039937\n",
            "loss : 3159.653807250562\n",
            "loss : 3052.308165718945\n",
            "loss : 3056.161624087285\n",
            "loss : 2967.7002800633372\n",
            "loss : 2982.5405343614902\n",
            "loss : 3135.1741540528265\n",
            "loss : 2957.794369045\n",
            "loss : 2967.0438970235723\n",
            "loss : 2914.792177593309\n",
            "loss : 2959.5719301434956\n",
            "loss : 2896.8682875447203\n",
            "loss : 2903.828604046462\n",
            "loss : 3023.181762316237\n",
            "loss : 2823.7607255846538\n",
            "loss : 3099.709869101233\n",
            "loss : 2948.495712683868\n",
            "loss : 2885.972779822239\n",
            "loss : 2918.132132661797\n",
            "loss : 2874.2057141401215\n",
            "loss : 2791.768351303466\n",
            "loss : 2855.175142191524\n",
            "loss : 2882.6888757759507\n",
            "loss : 2867.246527705381\n",
            "loss : 3103.3113583918416\n",
            "loss : 2911.38401269112\n",
            "loss : 2971.0129325318635\n",
            "loss : 2814.6164179187526\n",
            "loss : 2806.2079872313116\n",
            "loss : 2844.563278355722\n",
            "loss : 2678.7131165913784\n",
            "loss : 2753.4499789400797\n",
            "loss : 2720.6768924547187\n",
            "loss : 2718.833051057846\n",
            "loss : 2746.3768854870887\n",
            "loss : 2652.7953724616186\n",
            "loss : 2750.1959915063335\n",
            "loss : 2784.29320800377\n",
            "loss : 2870.3591360374053\n",
            "loss : 2705.423871768938\n",
            "loss : 2750.0974224303286\n",
            "loss : 2674.13685680355\n",
            "loss : 2651.9037126996755\n",
            "loss : 2639.3829746790175\n",
            "loss : 2588.1637097937846\n",
            "loss : 2572.402231322448\n",
            "loss : 2548.724082946762\n",
            "loss : 2687.6490770881555\n",
            "loss : 2556.8935717746645\n",
            "loss : 2595.7127848438313\n",
            "loss : 2725.6313263133134\n",
            "loss : 2625.18841609921\n",
            "loss : 2675.0280177143577\n",
            "loss : 2581.425449988253\n",
            "loss : 2549.794355466948\n",
            "loss : 2435.4891633835587\n",
            "loss : 2717.6352021881585\n",
            "loss : 2633.3137390968946\n",
            "loss : 2869.940965657274\n",
            "loss : 2652.985807223664\n",
            "loss : 2644.985137737747\n",
            "loss : 2572.49707501597\n",
            "loss : 2477.1976653271695\n",
            "loss : 2393.190348160475\n",
            "loss : 2467.4082930918776\n",
            "loss : 2470.0776230378397\n",
            "loss : 2356.962164638081\n",
            "loss : 2379.901620148201\n",
            "loss : 2452.581691905376\n",
            "loss : 2428.565705412072\n",
            "loss : 2450.0949598807492\n",
            "loss : 2577.4891066753426\n",
            "loss : 2422.0919973565856\n",
            "loss : 2508.441831148467\n",
            "loss : 2405.8531588163323\n",
            "loss : 2435.0886760413587\n",
            "loss : 2371.256772375389\n",
            "loss : 2472.9834029708472\n",
            "loss : 2236.952092844523\n",
            "loss : 2529.117505798266\n",
            "loss : 2377.298748671108\n",
            "loss : 2447.4798520354116\n",
            "loss : 2383.89390909349\n",
            "loss : 2467.9692764020465\n",
            "loss : 2337.4951918999295\n",
            "loss : 2391.2145830555373\n",
            "loss : 2253.8735058267325\n",
            "loss : 2287.20604522572\n",
            "loss : 2204.6487022000206\n",
            "loss : 2064.576921500657\n",
            "loss : 2372.6542176499256\n",
            "loss : 2344.6262534321413\n",
            "loss : 2270.7853297210895\n",
            "loss : 2354.241038517024\n",
            "loss : 2284.8672188747914\n",
            "loss : 2247.166890686699\n",
            "loss : 2160.20394367403\n",
            "loss : 2179.0313613607245\n",
            "loss : 2339.3646778289294\n",
            "loss : 2402.5874073206733\n",
            "loss : 2554.0590066650593\n",
            "loss : 2466.5098210207734\n",
            "loss : 2322.456375646578\n",
            "loss : 2245.9280473935437\n",
            "loss : 2201.520642636634\n",
            "loss : 2126.952439331272\n",
            "loss : 2108.581922371933\n",
            "loss : 2089.2049421318093\n",
            "loss : 2051.8400173911623\n",
            "loss : 2115.727767415926\n",
            "loss : 2180.9963836600164\n",
            "loss : 2233.641389455557\n",
            "loss : 1943.22953565528\n",
            "loss : 2072.4797755307695\n",
            "loss : 2141.4687451962754\n",
            "loss : 2089.122418645424\n",
            "loss : 2015.1933491135246\n",
            "loss : 2100.580356585704\n",
            "loss : 2076.4557845410945\n",
            "loss : 2101.1424325177413\n",
            "loss : 2061.7629946539373\n",
            "loss : 2056.317925965047\n",
            "loss : 2039.6497610121482\n",
            "loss : 1976.482102168838\n",
            "loss : 1907.9035685423332\n",
            "loss : 1950.111106967894\n",
            "loss : 1935.8614203263712\n",
            "loss : 1995.2529366331553\n",
            "loss : 1887.0795727739523\n",
            "loss : 2004.5992578334549\n",
            "loss : 2134.1639907605154\n",
            "loss : 2036.4902608678779\n",
            "loss : 1844.2732894227092\n",
            "loss : 1722.1959825292925\n",
            "loss : 1872.4805532467692\n",
            "loss : 1812.5358665338367\n",
            "loss : 2114.399242004634\n",
            "loss : 2289.480864239015\n",
            "loss : 2387.006108540079\n",
            "loss : 2173.358800561842\n",
            "loss : 2120.67913205727\n",
            "loss : 1932.30843750011\n",
            "loss : 1955.1909259574\n",
            "loss : 2049.9954178414027\n",
            "loss : 1835.9471983811245\n",
            "loss : 1859.6137822457968\n",
            "loss : 1672.4264289924872\n",
            "loss : 1852.1296818926885\n",
            "loss : 1885.6342001677235\n",
            "loss : 2033.5316251415875\n",
            "loss : 2107.597360005211\n",
            "loss : 1955.633589898795\n",
            "loss : 1864.9681780168066\n",
            "loss : 1849.6986063172621\n",
            "loss : 1892.2163103404212\n",
            "loss : 1684.7718529362053\n",
            "loss : 1867.9307440917246\n",
            "loss : 1758.259855463742\n",
            "loss : 1771.8792076812024\n",
            "loss : 1776.2186731912827\n",
            "loss : 1763.43213196359\n",
            "loss : 1767.6664087472745\n",
            "loss : 1725.9415964156042\n",
            "loss : 1711.7605993715515\n",
            "loss : 1882.3557996339798\n",
            "loss : 1823.3518665379402\n",
            "loss : 1850.3416685698585\n",
            "loss : 1713.1566023826795\n",
            "loss : 1785.2259715975904\n",
            "loss : 1686.2149342691825\n",
            "loss : 1870.7483796078964\n",
            "loss : 1701.423943254534\n",
            "loss : 1831.3545964184982\n",
            "loss : 1679.1902423133313\n",
            "loss : 1823.546164942263\n",
            "loss : 1748.9765199419692\n",
            "loss : 1822.3533073739432\n",
            "loss : 1961.0856529088392\n",
            "loss : 1780.9214709336447\n",
            "loss : 1818.4771297835123\n",
            "loss : 1765.2257860044292\n",
            "loss : 1905.9080379561105\n",
            "loss : 1710.5988027436815\n",
            "loss : 1635.7111568022428\n",
            "loss : 1810.6220889040533\n",
            "loss : 1654.3707080429685\n",
            "loss : 1620.0705138945118\n",
            "loss : 1625.6857134081056\n",
            "loss : 1817.1873914906687\n",
            "loss : 1635.0087502184224\n",
            "loss : 1538.154888586821\n",
            "loss : 1611.290991230453\n",
            "loss : 1754.161884841189\n",
            "loss : 1647.767286179996\n",
            "loss : 1624.0779943289006\n",
            "loss : 1527.6813812121986\n",
            "loss : 1649.2864251701353\n",
            "loss : 1691.1320096186005\n",
            "loss : 1638.5178162098373\n",
            "loss : 1845.8627985472212\n",
            "loss : 1872.3306346010997\n",
            "loss : 1938.3141564537623\n",
            "loss : 1804.7252069947904\n",
            "loss : 1675.14877437121\n",
            "loss : 1553.8607964342468\n",
            "loss : 1655.4112850777005\n",
            "loss : 1731.5839083640187\n",
            "loss : 1757.1352392638123\n",
            "loss : 1669.7963139250892\n",
            "loss : 1693.170775897649\n",
            "loss : 1627.2267483286769\n",
            "loss : 1700.635804251662\n",
            "loss : 1503.1836592235175\n",
            "loss : 1552.9139936703907\n",
            "loss : 1587.0238864257444\n",
            "loss : 1465.62614561335\n",
            "loss : 1447.9880915868698\n",
            "loss : 1559.9180059908572\n",
            "loss : 1592.1283327774568\n",
            "loss : 1526.249682045575\n",
            "loss : 1700.4656484695142\n",
            "loss : 1696.6372763434267\n",
            "loss : 1698.3054081626888\n",
            "loss : 1570.2212398829845\n",
            "loss : 1518.4939510356608\n",
            "loss : 1635.689131610987\n",
            "loss : 1603.8399267062157\n",
            "loss : 1482.2608985358675\n",
            "loss : 1483.7718914505565\n",
            "loss : 1443.037541183264\n",
            "loss : 1558.4569298182444\n",
            "loss : 1625.3231200291204\n",
            "loss : 1523.9106389274962\n",
            "loss : 1447.8124707289371\n",
            "loss : 1666.9292006969758\n",
            "loss : 1589.7152927247157\n",
            "loss : 1477.5106378075625\n",
            "loss : 1525.0510605628297\n",
            "loss : 1468.0278833928594\n",
            "loss : 1572.0626727296888\n",
            "loss : 1487.4237565997385\n",
            "loss : 1590.4137595141442\n",
            "loss : 1576.7997621484033\n",
            "loss : 1574.8973785702358\n",
            "loss : 1466.6272951608455\n",
            "loss : 1541.3786884093859\n",
            "loss : 1495.9675532825358\n",
            "loss : 1548.9090414860975\n",
            "loss : 1280.8373598072915\n",
            "loss : 1501.9621574156454\n",
            "loss : 1520.0698519763744\n",
            "loss : 1455.6591470015626\n",
            "loss : 1359.988534237354\n",
            "loss : 1456.00115782277\n",
            "loss : 1460.719270570155\n",
            "loss : 1393.1178873799286\n",
            "loss : 1494.057583177704\n",
            "loss : 1482.5328598044637\n",
            "loss : 1457.7203158676896\n",
            "loss : 1580.8382420461162\n",
            "loss : 1282.0169937622568\n",
            "loss : 1287.1346601843081\n",
            "loss : 1466.5468338583823\n",
            "loss : 1587.3185850866294\n",
            "loss : 1428.7596600996205\n",
            "loss : 1494.325584760611\n",
            "loss : 1380.3771968413453\n",
            "loss : 1546.7046474891936\n",
            "loss : 1601.126390516409\n",
            "loss : 1434.7843355094794\n",
            "loss : 1424.986018736029\n",
            "loss : 1302.8002607210708\n",
            "loss : 1246.8953612963828\n",
            "loss : 1268.7464657734636\n",
            "loss : 1437.7976280529501\n",
            "loss : 1379.279995244713\n",
            "loss : 1405.5092061648375\n",
            "loss : 1326.6876691694301\n",
            "loss : 1280.8819589160546\n",
            "loss : 1330.985547324305\n",
            "loss : 1274.1929546331967\n",
            "loss : 1254.5563335848842\n",
            "loss : 1360.61165225733\n",
            "loss : 1474.6766151700876\n",
            "loss : 1374.4175930852382\n",
            "loss : 1293.4687859168662\n",
            "loss : 1415.9110856846833\n",
            "loss : 1363.780232401963\n",
            "loss : 1308.834213323225\n",
            "loss : 1383.8669177740005\n",
            "loss : 1316.674470592783\n",
            "loss : 1416.4214174226531\n",
            "loss : 1499.7926234463052\n",
            "loss : 1351.875176351736\n",
            "loss : 1328.1531152104153\n",
            "loss : 1251.4789398590042\n",
            "loss : 1211.658059102756\n",
            "loss : 1346.196804659408\n",
            "loss : 1285.8687373300165\n",
            "loss : 1420.4665015957178\n",
            "loss : 1323.5839243138862\n",
            "loss : 1206.7361570162998\n",
            "loss : 1353.215940775928\n",
            "loss : 1194.4011693007524\n",
            "loss : 1281.5113590690298\n",
            "loss : 1327.2048787127096\n",
            "loss : 1321.2179173965783\n",
            "loss : 1281.7642638716868\n",
            "loss : 1220.3521454917243\n",
            "loss : 1268.1195498767192\n",
            "loss : 1381.062432223611\n",
            "loss : 1374.653177924214\n",
            "loss : 1515.3412840396543\n",
            "loss : 1337.866847558112\n",
            "loss : 1305.7931454791526\n",
            "loss : 1247.8117694818102\n",
            "loss : 1291.520746404557\n",
            "loss : 1253.4981591447931\n",
            "loss : 1181.4466525955504\n",
            "loss : 1310.7424601624575\n",
            "loss : 1326.78135935292\n",
            "loss : 1273.9690347728656\n",
            "loss : 1557.5123025649616\n",
            "loss : 1495.8904048399722\n",
            "loss : 1598.697779998282\n",
            "loss : 1367.1862416262675\n",
            "loss : 1216.9468998279153\n",
            "loss : 1271.6748879533372\n",
            "loss : 1193.4405929318364\n",
            "loss : 1397.8469291034453\n",
            "loss : 1308.7992824059675\n",
            "loss : 1279.1688195834727\n",
            "loss : 1230.0991592694813\n",
            "loss : 1316.3487838493093\n",
            "loss : 1217.9887390135818\n",
            "loss : 1358.7600025992244\n",
            "loss : 1245.5565312867993\n",
            "loss : 1175.819297895655\n",
            "loss : 1226.8026065123531\n",
            "loss : 1182.2557502976665\n",
            "loss : 1155.847415041526\n",
            "loss : 1194.1791531247227\n",
            "loss : 1240.891657960747\n",
            "loss : 1357.0796482499418\n",
            "loss : 1370.3959195415148\n",
            "loss : 1384.1563221238296\n",
            "loss : 1343.4835227115514\n",
            "loss : 1164.855722526572\n",
            "loss : 1141.4283426534491\n",
            "loss : 1341.5539847072373\n",
            "loss : 1233.6840594474684\n",
            "loss : 1382.6217272116583\n",
            "loss : 1270.7626835052915\n",
            "loss : 1215.7116101283982\n",
            "loss : 1352.2161460458806\n",
            "loss : 1059.614491090006\n",
            "loss : 1220.8213407994763\n",
            "loss : 1135.0230469108956\n",
            "loss : 1271.7520743623063\n",
            "loss : 1296.658954975345\n",
            "loss : 1284.5285161119946\n",
            "loss : 1191.9823318179097\n",
            "loss : 1225.154109742228\n",
            "loss : 1214.8288079640174\n",
            "loss : 1186.1826683311556\n",
            "loss : 1110.7257011626673\n",
            "loss : 1170.8234143003333\n",
            "loss : 1126.6036564757615\n",
            "loss : 1163.0536745052343\n",
            "loss : 1371.0600462279904\n",
            "loss : 1238.5429696111946\n",
            "loss : 1311.8170965064762\n",
            "loss : 1402.141848433606\n",
            "loss : 1462.0113083614958\n",
            "loss : 1376.5303397671912\n",
            "loss : 1336.9373396231078\n",
            "loss : 1311.1129605432338\n",
            "loss : 1309.4388360998616\n",
            "loss : 1288.233565076338\n",
            "loss : 1302.556673478804\n",
            "loss : 1333.3270885971522\n",
            "loss : 1408.016570806393\n",
            "loss : 1383.1987860084698\n",
            "loss : 1310.1551068397662\n",
            "loss : 1208.9908063466867\n",
            "loss : 1247.1309881393272\n",
            "loss : 1189.5537668539275\n",
            "loss : 1347.0019913975661\n",
            "loss : 1214.2552936701725\n",
            "loss : 1245.7280115709227\n",
            "loss : 1206.2923846820074\n",
            "loss : 1211.7733862015368\n",
            "loss : 1301.45220739556\n",
            "loss : 1381.6954811903151\n",
            "loss : 1381.1617033008224\n",
            "loss : 1188.9785537164491\n",
            "loss : 1243.0231826193897\n",
            "loss : 1353.194202466611\n",
            "loss : 1155.4710356429418\n",
            "loss : 1249.80627346858\n",
            "loss : 1205.9256561784123\n",
            "loss : 1201.3766869882857\n",
            "loss : 1120.0863057114718\n",
            "loss : 1168.3425479796501\n",
            "loss : 1186.1034908536988\n",
            "loss : 1183.4778032094287\n",
            "loss : 1124.4123349438798\n",
            "loss : 1214.4399401356072\n",
            "loss : 1105.407152209451\n",
            "loss : 1367.847876918089\n",
            "loss : 1111.2601736134939\n",
            "loss : 1040.932106855752\n",
            "loss : 1137.411558654793\n",
            "loss : 1102.4176292254447\n",
            "loss : 1104.707391871724\n",
            "loss : 1227.995834470082\n",
            "loss : 1175.768593858926\n",
            "loss : 1187.0207969113249\n",
            "loss : 1049.032787162993\n",
            "loss : 1199.0384590777908\n",
            "loss : 1041.2519805945058\n",
            "loss : 1122.0091656427205\n",
            "loss : 1158.5579401720292\n",
            "loss : 1214.159182558719\n",
            "loss : 1145.670798418922\n",
            "loss : 1170.775933900768\n",
            "loss : 1138.5116298673968\n",
            "loss : 1174.8153379452233\n",
            "loss : 1140.8714433553478\n",
            "loss : 1228.9476198204816\n",
            "loss : 1177.7845700063192\n",
            "loss : 1254.5133912139647\n",
            "loss : 1238.7397125385173\n",
            "loss : 1208.6928338148166\n",
            "loss : 1269.6616298935244\n",
            "loss : 1176.1597042981462\n",
            "loss : 1268.4842910350699\n",
            "loss : 1163.7832396372137\n",
            "loss : 1114.3519227095655\n",
            "loss : 1164.9036601981884\n",
            "loss : 1358.167071937891\n",
            "loss : 1276.9554566287466\n",
            "loss : 1192.3725961658881\n",
            "loss : 1274.6379999231108\n",
            "loss : 1106.1910803148294\n",
            "loss : 1171.4374193004714\n",
            "loss : 1214.1172802310361\n",
            "loss : 1191.620759359192\n",
            "loss : 1197.940430538637\n",
            "loss : 1274.7588066361827\n",
            "loss : 1233.5698496445173\n",
            "loss : 1241.4698736786086\n",
            "loss : 1175.992889610639\n",
            "loss : 1251.7561017786284\n",
            "loss : 1083.9003773833206\n",
            "loss : 1057.290845978987\n",
            "loss : 1269.1588771485817\n",
            "loss : 1109.4019401929734\n",
            "loss : 1011.5604149978044\n",
            "loss : 1069.9630774997163\n",
            "loss : 928.7556498459735\n",
            "loss : 1040.3767553101864\n",
            "loss : 965.2939772153612\n",
            "loss : 1089.1009531329055\n",
            "loss : 905.5063888972315\n",
            "loss : 1000.2242090797807\n",
            "loss : 1133.2715006103672\n",
            "loss : 998.6102189775117\n",
            "loss : 1003.771846149102\n",
            "loss : 1019.5964647913981\n",
            "loss : 1039.6846502509234\n",
            "loss : 1134.8932381564273\n",
            "loss : 1010.530397445259\n",
            "loss : 1007.0234719253511\n",
            "loss : 1010.6598983166325\n",
            "loss : 1033.6652758641549\n",
            "loss : 1014.4739393658751\n",
            "loss : 1104.1422474435417\n",
            "loss : 1046.4462176133907\n",
            "loss : 1041.6704361974475\n",
            "loss : 1096.5812311601367\n",
            "loss : 1136.3314835989172\n",
            "loss : 1009.6331452353697\n",
            "loss : 941.990875982516\n",
            "loss : 1033.7658831702272\n",
            "loss : 951.1748948687073\n",
            "loss : 1115.334917379923\n",
            "loss : 1083.6291057555763\n",
            "loss : 1088.320403736108\n",
            "loss : 1004.631791672359\n",
            "loss : 885.4889605685871\n",
            "loss : 1161.5093149150798\n",
            "loss : 1065.3369734573973\n",
            "loss : 1026.2565524986899\n",
            "loss : 1238.0418435411284\n",
            "loss : 1146.729299211329\n",
            "loss : 1016.4238771496284\n",
            "loss : 1069.8553592063017\n",
            "loss : 1025.9204533605798\n",
            "loss : 975.7756217986445\n",
            "loss : 1040.075928383137\n",
            "loss : 972.649152584662\n",
            "loss : 897.3761059752626\n",
            "loss : 928.4906034114612\n",
            "loss : 974.3843257563932\n",
            "loss : 1059.5560028628292\n",
            "loss : 1100.5480351514768\n",
            "loss : 943.1167673082332\n",
            "loss : 1091.7368285182479\n",
            "loss : 1072.837343552586\n",
            "loss : 1219.9833967539412\n",
            "loss : 1128.396868167852\n",
            "loss : 1138.7135506695613\n",
            "loss : 1060.4534707572384\n",
            "loss : 1185.1373086162907\n",
            "loss : 1059.6333955844675\n",
            "loss : 1236.7683437054566\n",
            "loss : 1158.9051419908812\n",
            "loss : 1052.5474302031255\n",
            "loss : 1126.7085031002225\n",
            "loss : 1020.870979390412\n",
            "loss : 970.4369339746816\n",
            "loss : 1084.3870645775855\n",
            "loss : 1069.0373477447843\n",
            "loss : 1143.5660411431713\n",
            "loss : 1081.1755643534943\n",
            "loss : 1066.1251538390748\n",
            "loss : 1211.1843964817267\n",
            "loss : 1077.9463634599313\n",
            "loss : 1084.791635446038\n",
            "loss : 1017.6478546019093\n",
            "loss : 1125.9704232884353\n",
            "loss : 1080.8192085439402\n",
            "loss : 1051.4887237100497\n",
            "loss : 1283.329587066216\n",
            "loss : 1017.1909332401706\n",
            "loss : 1023.4448248958192\n",
            "loss : 1097.9904326509463\n",
            "loss : 1015.9790942604668\n",
            "loss : 1085.6626805992928\n",
            "loss : 1006.4754997388297\n",
            "loss : 1121.2104854870042\n",
            "loss : 1033.5684555387202\n",
            "loss : 937.3364252181353\n",
            "loss : 1015.3104113266396\n",
            "loss : 965.8945040176604\n",
            "loss : 990.9371068511896\n",
            "loss : 830.9431057732297\n",
            "loss : 960.4954920097584\n",
            "loss : 1044.6121047932252\n",
            "loss : 1141.26210112929\n",
            "loss : 877.5063747536108\n",
            "loss : 1038.9476450207385\n",
            "loss : 1151.5327233523653\n",
            "loss : 1105.789663682204\n",
            "loss : 1095.8217111830904\n",
            "loss : 1080.4985944587936\n",
            "loss : 1013.0589114840967\n",
            "loss : 1021.5838327279448\n",
            "loss : 1142.2739614588795\n",
            "loss : 907.8581035328409\n",
            "loss : 1124.8194264842632\n",
            "loss : 1087.4383118691458\n",
            "loss : 1037.9822910668104\n",
            "loss : 926.6508692434186\n",
            "loss : 1033.0893348565914\n",
            "loss : 1056.9857013420967\n",
            "loss : 1083.9195873263252\n",
            "loss : 1152.2260825404123\n",
            "loss : 1042.407274675263\n",
            "loss : 1097.2618505816254\n",
            "loss : 982.709192169543\n",
            "loss : 959.100910429984\n",
            "loss : 968.7424027360205\n",
            "loss : 1043.4620268470844\n",
            "loss : 913.76203914022\n",
            "loss : 975.3774042019645\n",
            "loss : 956.2016286617236\n",
            "loss : 1029.6552929781383\n",
            "loss : 1035.2512734325687\n",
            "loss : 1029.967276797257\n",
            "loss : 887.6880474558347\n",
            "loss : 914.8903297274044\n",
            "loss : 856.4772514373321\n",
            "loss : 809.7761849005819\n",
            "loss : 996.5784920545892\n",
            "loss : 1119.3132730896104\n",
            "loss : 1126.6313369284312\n",
            "loss : 1155.5230278662884\n",
            "loss : 949.5443026252649\n",
            "loss : 996.7141636950398\n",
            "loss : 1025.4231228806636\n",
            "loss : 1049.6281054953397\n",
            "loss : 1018.2021740666949\n",
            "loss : 952.7771457621496\n",
            "loss : 1040.7163881576148\n",
            "loss : 995.0436895691953\n",
            "loss : 1074.9649631818\n",
            "loss : 1068.8136376353282\n",
            "loss : 1116.1259191100908\n",
            "loss : 998.2638684479396\n",
            "loss : 993.005344516135\n",
            "loss : 1096.6704759642666\n",
            "loss : 1010.558306375912\n",
            "loss : 1016.8331353896872\n",
            "loss : 887.1638318808384\n",
            "loss : 911.8086252084531\n",
            "loss : 1002.1789553205529\n",
            "loss : 1005.546811403269\n",
            "loss : 996.8127109305192\n",
            "loss : 882.7639816895121\n",
            "loss : 953.685691894219\n",
            "loss : 932.7760121244971\n",
            "loss : 1108.2084602675184\n",
            "loss : 1072.8917511808231\n",
            "loss : 965.3025688696672\n",
            "loss : 979.6268369345548\n",
            "loss : 885.6863729672984\n",
            "loss : 1004.4633647481407\n",
            "loss : 938.5795760929636\n",
            "loss : 1089.8641062125125\n",
            "loss : 884.7807702611178\n",
            "loss : 1047.309936178678\n",
            "loss : 980.6660656425804\n",
            "loss : 987.4284420319882\n",
            "loss : 961.9412013706965\n",
            "loss : 1043.526362266798\n",
            "loss : 1039.560362157768\n",
            "loss : 967.1315664902093\n",
            "loss : 1047.1700823116475\n",
            "loss : 851.3174441767696\n",
            "loss : 959.2902648820342\n",
            "loss : 915.8826212109461\n",
            "loss : 876.7507585280848\n",
            "loss : 851.8003923552583\n",
            "loss : 952.3803101345607\n",
            "loss : 958.3689479661882\n",
            "loss : 937.5501518462627\n",
            "loss : 969.2099484529056\n",
            "loss : 939.804675497923\n",
            "loss : 1053.5402169901167\n",
            "loss : 966.5370002784147\n",
            "loss : 1050.3996610561476\n",
            "loss : 909.6235246238664\n",
            "loss : 900.6929335230277\n",
            "loss : 961.1406887727045\n",
            "loss : 1029.4681218539934\n",
            "loss : 918.2011177470065\n",
            "loss : 880.003034018632\n",
            "loss : 915.1395122587705\n",
            "loss : 984.3887130662417\n",
            "loss : 881.516458518585\n",
            "loss : 788.1903045950733\n",
            "loss : 907.102799775666\n",
            "loss : 924.3434221210368\n",
            "loss : 893.9342462410426\n",
            "loss : 943.9000929274206\n",
            "loss : 917.145747719345\n",
            "loss : 1040.989383707446\n",
            "loss : 914.1993377609764\n",
            "loss : 935.601487141166\n",
            "loss : 985.8721205128638\n",
            "loss : 833.6694438832095\n",
            "loss : 891.6935579585847\n",
            "loss : 955.5083191820586\n",
            "loss : 1020.0519047068835\n",
            "loss : 979.282836149636\n",
            "loss : 911.1454297815263\n",
            "loss : 928.1942711805923\n",
            "loss : 1038.5560072033036\n",
            "loss : 1008.5170345989188\n",
            "loss : 1011.7236838036013\n",
            "loss : 1028.8105123385874\n",
            "loss : 1105.2266195660877\n",
            "loss : 1065.494686422612\n",
            "loss : 981.948549504743\n",
            "loss : 931.8070848209121\n",
            "loss : 852.4153695903371\n",
            "loss : 1016.1305824475768\n",
            "loss : 1025.39256951077\n",
            "loss : 990.5146156464302\n",
            "loss : 872.7297100834921\n",
            "loss : 932.2787257261332\n",
            "loss : 909.2476743209718\n",
            "loss : 868.3172755867365\n",
            "loss : 976.0471640916165\n",
            "loss : 883.3700205664381\n",
            "loss : 938.7186815432306\n",
            "loss : 983.4680530088783\n",
            "loss : 987.4807228507692\n",
            "loss : 900.5441814067605\n",
            "loss : 914.3175829295892\n",
            "loss : 1135.3891548317292\n",
            "loss : 1014.4188998959849\n",
            "loss : 918.14776839574\n",
            "loss : 940.8464602417496\n",
            "loss : 942.6813709274215\n",
            "loss : 876.7824262964464\n",
            "loss : 941.5837993870655\n",
            "loss : 928.5261963280939\n",
            "loss : 1000.0191682984073\n",
            "loss : 890.0291268349793\n",
            "loss : 989.2629350983335\n",
            "loss : 887.4299436190392\n",
            "loss : 948.8789939857236\n",
            "loss : 886.0643753315021\n",
            "loss : 901.1593229122101\n",
            "loss : 875.45615065677\n",
            "loss : 880.2546231123225\n",
            "loss : 859.3574418605092\n",
            "loss : 931.3197279234398\n",
            "loss : 904.4630996606172\n",
            "loss : 952.699591375052\n",
            "loss : 865.7501208968434\n",
            "loss : 906.6048119872187\n",
            "loss : 777.9864286485333\n",
            "loss : 816.0494437333884\n",
            "loss : 931.9460911766535\n",
            "loss : 889.5199276435354\n",
            "loss : 869.1110268757845\n",
            "loss : 857.0603966497627\n",
            "loss : 1020.6596515479629\n",
            "loss : 896.7025838955315\n",
            "loss : 913.6481968698951\n",
            "loss : 955.5400282443812\n",
            "loss : 825.3043209933819\n",
            "loss : 923.0244498406896\n",
            "loss : 844.5767372318467\n",
            "loss : 1000.1473515239394\n",
            "loss : 968.6225790039273\n",
            "loss : 829.7399847566354\n",
            "loss : 881.8227174104691\n",
            "loss : 894.6376907341514\n",
            "loss : 889.3304919169743\n",
            "loss : 922.1235165345832\n",
            "loss : 865.6568791081503\n",
            "loss : 913.7576399088873\n",
            "loss : 943.8467945017642\n",
            "loss : 835.5693437809455\n",
            "loss : 937.8346802201634\n",
            "loss : 686.3727548333729\n",
            "loss : 950.6418218969907\n",
            "loss : 788.2901826348264\n",
            "loss : 921.9272552352629\n",
            "loss : 955.1304420100494\n",
            "loss : 841.1057831414373\n",
            "loss : 850.4893871375135\n",
            "loss : 898.5156026301681\n",
            "loss : 869.1277426224458\n",
            "loss : 776.2423839567558\n",
            "loss : 867.2144429704515\n",
            "loss : 881.5781155506747\n",
            "loss : 873.1139621815507\n",
            "loss : 859.3112736964881\n",
            "loss : 872.9245130075343\n",
            "loss : 800.5859276080778\n",
            "loss : 877.8879053955079\n",
            "loss : 944.4438553663043\n",
            "loss : 819.1213326488204\n",
            "loss : 920.8535057343826\n",
            "loss : 878.4773184672606\n",
            "loss : 846.3391414547285\n",
            "loss : 799.8998072762205\n",
            "loss : 931.672140132378\n",
            "loss : 929.7521994912189\n",
            "loss : 947.1919657085256\n",
            "loss : 890.2734795355155\n",
            "loss : 834.8991245376501\n",
            "loss : 817.0448905581137\n",
            "loss : 815.232340404683\n",
            "loss : 934.6066109548644\n",
            "loss : 819.8894963184235\n",
            "loss : 990.2040204207634\n",
            "loss : 963.5604400930026\n",
            "loss : 850.5400320545771\n",
            "loss : 970.1072160644479\n",
            "loss : 1033.1484849012363\n",
            "loss : 924.9890895008059\n",
            "loss : 802.7093563828209\n",
            "loss : 879.9627276489066\n",
            "loss : 739.9752688143536\n",
            "loss : 886.2036789656561\n",
            "loss : 977.1666779692104\n",
            "loss : 832.7366775782824\n",
            "loss : 828.9133166487924\n",
            "loss : 791.1218882076305\n",
            "loss : 881.1122254551628\n",
            "loss : 733.9449312488766\n",
            "loss : 795.4769735123386\n",
            "loss : 838.2658287875271\n",
            "loss : 777.8861484735535\n",
            "loss : 745.0513374770496\n",
            "loss : 827.3193139198472\n",
            "loss : 898.6894608341188\n",
            "loss : 798.3015876631139\n",
            "loss : 834.7502382856553\n",
            "loss : 957.6368258498786\n",
            "loss : 835.0498697914489\n",
            "loss : 878.3001362928019\n",
            "loss : 981.8965977019366\n",
            "loss : 755.4091122091193\n",
            "loss : 798.9269645602704\n",
            "loss : 941.6897710625836\n",
            "loss : 952.0040657454059\n",
            "loss : 901.751679767952\n",
            "loss : 803.143555223766\n",
            "loss : 797.0497329722348\n",
            "loss : 853.9134159172328\n",
            "loss : 829.2683895147236\n",
            "loss : 755.1331301102676\n",
            "loss : 853.012847198291\n",
            "loss : 962.4292009387469\n",
            "loss : 859.1707393079427\n",
            "loss : 753.0272590791819\n",
            "loss : 822.7810948750717\n",
            "loss : 804.3033111079997\n",
            "loss : 767.5021320038741\n",
            "loss : 724.3564641519924\n",
            "loss : 925.867532262957\n",
            "loss : 937.6887056045821\n",
            "loss : 808.6041360617369\n",
            "loss : 872.8158583024657\n",
            "loss : 752.0201092843593\n",
            "loss : 867.402465509251\n",
            "loss : 756.9390256336083\n",
            "loss : 825.1721457899283\n",
            "loss : 1065.3235968937338\n",
            "loss : 937.3760128285878\n",
            "loss : 863.1283818217539\n",
            "loss : 819.8705035051828\n",
            "loss : 878.5341264987167\n",
            "loss : 834.0758050576756\n",
            "loss : 806.8897820752708\n",
            "loss : 854.1336929053577\n",
            "loss : 808.1674809701979\n",
            "loss : 824.283284385549\n",
            "loss : 847.3633696488308\n",
            "loss : 875.7147280089799\n",
            "loss : 909.5727768965827\n",
            "loss : 994.2948119811425\n",
            "loss : 908.4569809517975\n",
            "loss : 748.2673981106939\n",
            "loss : 771.024652534549\n",
            "loss : 680.6304549188503\n",
            "loss : 849.360239103618\n",
            "loss : 810.2312508461141\n",
            "loss : 833.979589473505\n",
            "loss : 831.7479435795126\n",
            "loss : 787.8464111606572\n",
            "loss : 826.7358359182412\n",
            "loss : 725.1671651478442\n",
            "loss : 852.9523930790012\n",
            "loss : 837.5654909797927\n",
            "loss : 778.1551955215186\n",
            "loss : 730.9778845178232\n",
            "loss : 729.4043523725014\n",
            "loss : 773.5599626418893\n",
            "loss : 788.4775474771712\n",
            "loss : 699.1219689923491\n",
            "loss : 887.1628672162585\n",
            "loss : 813.0000538674177\n",
            "loss : 707.2586643863742\n",
            "loss : 836.1395784290813\n",
            "loss : 943.6092396421491\n",
            "loss : 891.048144602463\n",
            "loss : 950.4737425228454\n",
            "loss : 849.6586568588539\n",
            "loss : 771.5558191074967\n",
            "loss : 681.0237896701145\n",
            "loss : 813.1858999956705\n",
            "loss : 817.7125204640138\n",
            "loss : 768.5002864371698\n",
            "loss : 816.5386345520998\n",
            "loss : 793.0916644698186\n",
            "loss : 703.7755138809803\n",
            "loss : 731.695675379046\n",
            "loss : 805.4390712006535\n",
            "loss : 845.6874206479876\n",
            "loss : 816.0462090705945\n",
            "loss : 801.1394494455967\n",
            "loss : 801.6984296930391\n",
            "loss : 803.9904555339082\n",
            "loss : 742.3916069992573\n",
            "loss : 765.9447776784375\n",
            "loss : 860.0311230030462\n",
            "loss : 732.493033867396\n",
            "loss : 890.2082161258304\n",
            "loss : 923.017006527198\n",
            "loss : 923.1421785250213\n",
            "loss : 952.812617626988\n",
            "loss : 673.6453425723861\n",
            "loss : 760.8816526317166\n",
            "loss : 893.6744684108282\n",
            "loss : 883.6752560441303\n",
            "loss : 866.6246415913215\n",
            "loss : 754.0165422526482\n",
            "loss : 844.8973539119027\n",
            "loss : 882.481635774284\n",
            "loss : 859.3624470065304\n",
            "loss : 776.3784923201762\n",
            "loss : 672.9383956013917\n",
            "loss : 702.7174670078614\n",
            "loss : 796.9480346554553\n",
            "loss : 890.4432709265957\n",
            "loss : 785.6142452017289\n",
            "loss : 810.8764249087105\n",
            "loss : 698.7363586313433\n",
            "loss : 748.4302993040217\n",
            "loss : 703.3414820610951\n",
            "loss : 728.5642723734475\n",
            "loss : 720.6956050781042\n",
            "loss : 819.2685186845924\n",
            "loss : 762.4453701337541\n",
            "loss : 627.5498361691355\n",
            "loss : 781.6624683834964\n",
            "loss : 744.8426141872164\n",
            "loss : 820.6773171421748\n",
            "loss : 787.5439213426645\n",
            "loss : 761.9023249822463\n",
            "loss : 717.8476476939255\n",
            "loss : 643.7945228201725\n",
            "loss : 713.718770506016\n",
            "loss : 718.6240746159689\n",
            "loss : 701.7681768765243\n",
            "loss : 710.9049176815917\n",
            "loss : 791.8868592610252\n",
            "loss : 926.0211282323372\n",
            "loss : 866.6286293193139\n",
            "loss : 709.6335540250719\n",
            "loss : 745.6842085470728\n",
            "loss : 756.5444796574978\n",
            "loss : 667.5624052514718\n",
            "loss : 817.6129891493252\n",
            "loss : 737.6560515401811\n",
            "loss : 780.3316553996895\n",
            "loss : 724.8433947218782\n",
            "loss : 706.5104325944113\n",
            "loss : 712.7374400473302\n",
            "loss : 858.389820156515\n",
            "loss : 688.5143311250022\n",
            "loss : 666.0436759061251\n",
            "loss : 741.1110885210286\n",
            "loss : 649.9986245981918\n",
            "loss : 685.1778207669956\n",
            "loss : 814.2033666490985\n",
            "loss : 722.5365434759112\n",
            "loss : 671.7165413470711\n",
            "loss : 681.5745937883021\n",
            "loss : 848.6783742074247\n",
            "loss : 843.6914538437986\n",
            "loss : 813.5803208965028\n",
            "loss : 977.0448366137894\n",
            "loss : 772.1281614090076\n",
            "loss : 763.4364545608571\n",
            "loss : 809.2380788346768\n",
            "loss : 716.9936158252063\n",
            "loss : 779.239377621866\n",
            "loss : 648.741195929226\n",
            "loss : 762.5208718646779\n",
            "loss : 702.9757572725619\n",
            "loss : 737.7962660339014\n",
            "loss : 767.780490504287\n",
            "loss : 680.9605177881961\n",
            "loss : 764.5377920747627\n",
            "loss : 746.252431099826\n",
            "loss : 635.7596092575141\n",
            "loss : 693.8864817519388\n",
            "loss : 744.1007375833551\n",
            "loss : 729.3388732051001\n",
            "loss : 741.319559845738\n",
            "loss : 816.9829938512341\n",
            "loss : 807.2790489637507\n",
            "loss : 833.4833424556982\n",
            "loss : 912.7445199397255\n",
            "loss : 732.1493550677258\n",
            "loss : 856.0358633647105\n",
            "loss : 832.6605786362784\n",
            "loss : 754.757220110393\n",
            "loss : 805.1122509995631\n",
            "loss : 840.1219101385275\n",
            "loss : 713.4215684295127\n",
            "loss : 862.8707272820827\n",
            "loss : 873.3750523616721\n",
            "loss : 653.7699178127425\n",
            "loss : 738.6057277193471\n",
            "loss : 681.3784300784155\n",
            "loss : 831.8746958275641\n",
            "loss : 769.3640582721521\n",
            "loss : 758.9421784805002\n",
            "loss : 778.6172116812046\n",
            "loss : 760.8900094610209\n",
            "loss : 707.1147303227044\n",
            "loss : 851.755499037117\n",
            "loss : 830.464886923128\n",
            "loss : 741.9562049289378\n",
            "loss : 680.5589943244571\n",
            "loss : 722.5903764821353\n",
            "loss : 721.1371854234226\n",
            "loss : 772.4236209323045\n",
            "loss : 695.8263169273403\n",
            "loss : 897.9261389646701\n",
            "loss : 768.4958589346222\n",
            "loss : 745.1410765862444\n",
            "loss : 781.7215098528752\n",
            "loss : 782.1846624937177\n",
            "loss : 691.9700336219476\n",
            "loss : 723.1246643252523\n",
            "loss : 704.2608317138607\n",
            "loss : 825.0393254923554\n",
            "loss : 869.0092503123475\n",
            "loss : 809.1651106104085\n",
            "loss : 749.2920781633675\n",
            "loss : 777.1618644931281\n",
            "loss : 892.6568463971286\n",
            "loss : 863.923514493825\n",
            "loss : 806.361467428909\n",
            "loss : 717.2609822012312\n",
            "loss : 760.595362501072\n",
            "loss : 788.2832370049596\n",
            "loss : 821.0429584574963\n",
            "loss : 761.2960102996813\n",
            "loss : 738.8715868943856\n",
            "loss : 776.9374462195668\n",
            "loss : 740.0411370034549\n",
            "loss : 779.503889156749\n",
            "loss : 617.5252169346662\n",
            "loss : 760.0397558735131\n",
            "loss : 799.2473168264557\n",
            "loss : 753.8553543258009\n",
            "loss : 675.139425793488\n",
            "loss : 716.7807670344649\n",
            "loss : 740.0163684289453\n",
            "loss : 833.5244950732463\n",
            "loss : 771.2739441989788\n",
            "loss : 799.500265989753\n",
            "loss : 749.8212797868015\n",
            "loss : 717.5383454529292\n",
            "loss : 658.6123132902842\n",
            "loss : 721.7764489615714\n",
            "loss : 744.203069580992\n",
            "loss : 823.942916906623\n",
            "loss : 763.6045787115769\n",
            "loss : 786.955961834575\n",
            "loss : 659.116543614609\n",
            "loss : 712.413208777404\n",
            "loss : 736.8419987293478\n",
            "loss : 728.681678902564\n",
            "loss : 741.472022580198\n",
            "loss : 918.8030246759599\n",
            "loss : 720.3913600796382\n",
            "loss : 732.5379690511752\n",
            "loss : 638.3547045380567\n",
            "loss : 682.5327357870199\n",
            "loss : 678.3507316669616\n",
            "loss : 789.1759910682415\n",
            "loss : 691.5912284542278\n",
            "loss : 767.7039524895538\n",
            "loss : 775.077789884939\n",
            "loss : 810.1530224882063\n",
            "loss : 762.0316740302901\n",
            "loss : 960.7334704060296\n",
            "loss : 1092.1421895962558\n",
            "loss : 1007.3363126772188\n",
            "loss : 654.9805937817058\n",
            "loss : 781.519336525774\n",
            "loss : 681.1626000421084\n",
            "loss : 737.9190577176967\n",
            "loss : 676.1270332669599\n",
            "loss : 645.1012102139302\n",
            "loss : 669.7024573384974\n",
            "loss : 642.4230155355785\n",
            "loss : 668.018701564159\n",
            "loss : 798.2279724127741\n",
            "loss : 621.248182289696\n",
            "loss : 750.8282497257854\n",
            "loss : 780.0115395728378\n",
            "loss : 609.8564458196865\n",
            "loss : 711.49381484423\n",
            "loss : 646.5251304851768\n",
            "loss : 628.7906244555129\n",
            "loss : 839.5034014535479\n",
            "loss : 694.2528742019721\n",
            "loss : 802.7017891263948\n",
            "loss : 673.9401701784965\n",
            "loss : 702.5545165052379\n",
            "loss : 651.8034426006286\n",
            "loss : 658.8976381510886\n",
            "loss : 750.3735781803371\n",
            "loss : 718.6646239152878\n",
            "loss : 816.6182817756928\n",
            "loss : 843.6501309019989\n",
            "loss : 788.7427268995716\n",
            "loss : 713.380703131162\n",
            "loss : 735.8587493762169\n",
            "loss : 655.4398828190288\n",
            "loss : 664.9842587757871\n",
            "loss : 620.5248728958057\n",
            "loss : 762.1735603062083\n",
            "loss : 809.0276967672988\n",
            "loss : 769.719975382686\n",
            "loss : 761.5347967434125\n",
            "loss : 723.18788432534\n",
            "loss : 652.3417147384937\n",
            "loss : 641.5825535137946\n",
            "loss : 677.1755784471866\n",
            "loss : 831.4703624279737\n",
            "loss : 705.9525865530958\n",
            "loss : 675.8607682568838\n",
            "loss : 604.396701218493\n",
            "loss : 695.1968693211212\n",
            "loss : 670.442303023018\n",
            "loss : 776.5748327450691\n",
            "loss : 722.3517145237108\n",
            "loss : 751.4022865553043\n",
            "loss : 750.8432067548256\n",
            "loss : 705.3445963802913\n",
            "loss : 606.3033627778862\n",
            "loss : 684.7574513467473\n",
            "loss : 662.9775405539117\n",
            "loss : 725.3822765461091\n",
            "loss : 684.2297320656397\n",
            "loss : 686.0103933572725\n",
            "loss : 745.922354423873\n",
            "loss : 744.2959884868834\n",
            "loss : 717.5979954852064\n",
            "loss : 751.7081659436908\n",
            "loss : 786.7676268461666\n",
            "loss : 701.9313386417317\n",
            "loss : 717.3142106881032\n",
            "loss : 668.5130663258035\n",
            "loss : 711.5253336162025\n",
            "loss : 545.3674776669276\n",
            "loss : 638.594961487157\n",
            "loss : 716.4069523343805\n",
            "loss : 741.8390144785392\n",
            "loss : 624.8569153266915\n",
            "loss : 617.0003637478259\n",
            "loss : 572.7565335890681\n",
            "loss : 685.3564440049117\n",
            "loss : 677.693441295797\n",
            "loss : 734.5222056357948\n",
            "loss : 813.960880545365\n",
            "loss : 795.8588556563038\n",
            "loss : 765.7324681857178\n",
            "loss : 688.7393861716725\n",
            "loss : 753.5106651607214\n",
            "loss : 941.6865346466421\n",
            "loss : 680.9767633315685\n",
            "loss : 744.1404816894941\n",
            "loss : 789.7037826859168\n",
            "loss : 855.9530929953482\n",
            "loss : 820.8776869575233\n",
            "loss : 797.0863141466918\n",
            "loss : 860.0792429392786\n",
            "loss : 786.7611072739086\n",
            "loss : 682.3077167609538\n",
            "loss : 715.8631437235074\n",
            "loss : 780.4233186026744\n",
            "loss : 661.4922022136197\n",
            "loss : 718.4174834578466\n",
            "loss : 717.7854176286971\n",
            "loss : 705.8977605551853\n",
            "loss : 746.2341462542087\n",
            "loss : 678.6483631623141\n",
            "loss : 706.3185203567577\n",
            "loss : 725.9663345081133\n",
            "loss : 751.6547904253339\n",
            "loss : 679.3303464163657\n",
            "loss : 806.6037156835289\n",
            "loss : 724.6792361554055\n",
            "loss : 783.6040567515263\n",
            "loss : 739.4356405361264\n",
            "loss : 720.9579257418291\n",
            "loss : 695.6647833734412\n",
            "loss : 599.4197237084338\n",
            "loss : 765.5765510919501\n",
            "loss : 657.7787694682843\n",
            "loss : 714.9663057290143\n",
            "loss : 851.2846000877158\n",
            "loss : 688.197205461471\n",
            "loss : 693.3304642316832\n",
            "loss : 824.0298781256779\n",
            "loss : 676.3967461099337\n",
            "loss : 786.904052844062\n",
            "loss : 760.9041288183358\n",
            "loss : 784.6599208640775\n",
            "loss : 805.0313807078128\n",
            "loss : 777.6689570621221\n",
            "loss : 687.858453173784\n",
            "loss : 708.3188730019923\n",
            "loss : 600.3431136440694\n",
            "loss : 681.0507366602259\n",
            "loss : 601.1926378220401\n",
            "loss : 711.0001033596809\n",
            "loss : 672.4943990025383\n",
            "loss : 770.6065791745243\n",
            "loss : 783.6350320069791\n",
            "loss : 832.1322152586084\n",
            "loss : 799.0512963763005\n",
            "loss : 730.6460819417025\n",
            "loss : 773.0426915227911\n",
            "loss : 841.9472587667144\n",
            "loss : 815.6983108842834\n",
            "loss : 729.2459498538141\n",
            "loss : 707.9005192759532\n",
            "loss : 626.0720413819574\n",
            "loss : 624.2204577243099\n",
            "loss : 718.6982483937293\n",
            "loss : 657.1999547857624\n",
            "loss : 746.0371574982913\n",
            "loss : 668.0856350019327\n",
            "loss : 609.816997069616\n",
            "loss : 660.8304276731794\n",
            "loss : 683.3298771648996\n",
            "loss : 698.9841423395035\n",
            "loss : 741.5100336733778\n",
            "loss : 677.8587111379466\n",
            "loss : 656.3485228663294\n",
            "loss : 611.1103028667661\n",
            "loss : 648.7650159517128\n",
            "loss : 807.2673243186088\n",
            "loss : 599.0077849812825\n",
            "loss : 723.9094622719731\n",
            "loss : 711.6028385574223\n",
            "loss : 710.2378288144716\n",
            "loss : 541.7901984308846\n",
            "loss : 748.7313037638419\n",
            "loss : 755.6061660928215\n",
            "loss : 768.075485719975\n",
            "loss : 651.636612782967\n",
            "loss : 658.8024786942576\n",
            "loss : 660.0776070988356\n",
            "loss : 707.8316018691869\n",
            "loss : 693.2601085400179\n",
            "loss : 582.6128903622421\n",
            "loss : 684.7394873661652\n",
            "loss : 550.4215120553515\n",
            "loss : 594.650063112511\n",
            "loss : 606.6657584841937\n",
            "loss : 595.6741988154472\n",
            "loss : 678.7297274512091\n",
            "loss : 570.1724754221982\n",
            "loss : 544.0873685023316\n",
            "loss : 592.0524490119153\n",
            "loss : 591.2815720094075\n",
            "loss : 626.5830478471469\n",
            "loss : 506.08693939042047\n",
            "loss : 624.9840325712686\n",
            "loss : 699.953886889031\n",
            "loss : 692.8096591478472\n",
            "loss : 697.4821881179364\n",
            "loss : 678.3166721929827\n",
            "loss : 508.5098876080191\n",
            "loss : 650.0473338551665\n",
            "loss : 638.356418521141\n",
            "loss : 666.8933278613874\n",
            "loss : 755.3971949037089\n",
            "loss : 694.3438885501214\n",
            "loss : 587.8308120271963\n",
            "loss : 638.6081472870158\n",
            "loss : 663.9106909824769\n",
            "loss : 674.9881161476944\n",
            "loss : 660.0599478516144\n",
            "loss : 698.3177177144726\n",
            "loss : 622.9001793207685\n",
            "loss : 655.8049554923275\n",
            "loss : 562.20017782627\n",
            "loss : 549.7147794234145\n",
            "loss : 625.1199865901119\n",
            "loss : 622.2774906074651\n",
            "loss : 646.9511975025657\n",
            "loss : 655.7418938010919\n",
            "loss : 689.9138983413927\n",
            "loss : 709.6236139010929\n",
            "loss : 716.727278938434\n",
            "loss : 712.8546672383596\n",
            "loss : 573.9656222006141\n",
            "loss : 719.9574308658348\n",
            "loss : 644.2905674899357\n",
            "loss : 672.8535458527497\n",
            "loss : 707.4079724062914\n",
            "loss : 694.7155760256815\n",
            "loss : 752.831484419303\n",
            "loss : 654.7137763868686\n",
            "loss : 570.7703311715858\n",
            "loss : 627.1717253408202\n",
            "loss : 660.6447038818874\n",
            "loss : 698.5962782105951\n",
            "loss : 663.9727214610443\n",
            "loss : 621.6573941831028\n",
            "loss : 647.6026287329152\n",
            "loss : 541.1462196677221\n",
            "loss : 714.3035942586221\n",
            "loss : 567.1048332200652\n",
            "loss : 590.8600780669808\n",
            "loss : 743.7506884980107\n",
            "loss : 682.6781421603748\n",
            "loss : 659.6060641599552\n",
            "loss : 759.1807248496075\n",
            "loss : 753.1591315726197\n",
            "loss : 703.3024823106533\n",
            "loss : 720.3416821774993\n",
            "loss : 714.0561441097193\n",
            "loss : 594.0518581250245\n",
            "loss : 573.9986877698685\n",
            "loss : 555.8346734121081\n",
            "loss : 627.0892545853189\n",
            "loss : 544.6881697758829\n",
            "loss : 715.8545559554177\n",
            "loss : 708.351886954634\n",
            "loss : 704.5013034061383\n",
            "loss : 647.1486277407508\n",
            "loss : 652.1762043807021\n",
            "loss : 691.3809845543501\n",
            "loss : 696.0220585682771\n",
            "loss : 599.952391737937\n",
            "loss : 615.6819800308474\n",
            "loss : 701.0699341287661\n",
            "loss : 680.8678370901275\n",
            "loss : 567.4796607112669\n",
            "loss : 528.2975863232425\n",
            "loss : 639.2509830606438\n",
            "loss : 575.5418584446638\n",
            "loss : 618.5841705994167\n",
            "loss : 558.6623230750511\n",
            "loss : 703.1729802714867\n",
            "loss : 700.678600928182\n",
            "loss : 683.3424137560953\n",
            "loss : 660.4448912137755\n",
            "loss : 622.7581007667186\n",
            "loss : 712.5659482281749\n",
            "loss : 608.8140436640938\n",
            "loss : 612.798472668888\n",
            "loss : 670.5718323423007\n",
            "loss : 658.3351856529237\n",
            "loss : 690.2758424207493\n",
            "loss : 739.0803591498654\n",
            "loss : 705.0870305807877\n",
            "loss : 695.4422784358244\n",
            "loss : 668.3576257333041\n",
            "loss : 547.7983169666063\n",
            "loss : 681.0065419992166\n",
            "loss : 604.1571397713958\n",
            "loss : 580.4292335217875\n",
            "loss : 633.1242205203638\n",
            "loss : 601.5620699640946\n",
            "loss : 585.5942822798113\n",
            "loss : 594.2148226423283\n",
            "loss : 513.7234341427834\n",
            "loss : 630.906455231597\n",
            "loss : 657.1041942435558\n",
            "loss : 637.4873096087307\n",
            "loss : 682.2662342005603\n",
            "loss : 603.1454518315138\n",
            "loss : 525.845128501556\n",
            "loss : 649.3456675834543\n",
            "loss : 538.5501707167068\n",
            "loss : 712.5895805770285\n",
            "loss : 642.657951019352\n",
            "loss : 614.0069074787644\n",
            "loss : 598.9626283792802\n",
            "loss : 609.4548189248956\n",
            "loss : 614.221021757948\n",
            "loss : 753.6547928406635\n",
            "loss : 700.7898098524859\n",
            "loss : 620.1297774400014\n",
            "loss : 579.9332033397718\n",
            "loss : 704.1439870323342\n",
            "loss : 678.2828316477\n",
            "loss : 667.0536351827965\n",
            "loss : 630.6454478704528\n",
            "loss : 581.1134029751416\n",
            "loss : 556.293222701461\n",
            "loss : 763.7849128172851\n",
            "loss : 759.1262340065655\n",
            "loss : 784.4456854265889\n",
            "loss : 692.748133624171\n",
            "loss : 651.0988304195015\n",
            "loss : 731.5005548362653\n",
            "loss : 693.6850791006773\n",
            "loss : 697.9958289739338\n",
            "loss : 698.7433606719248\n",
            "loss : 639.3189221312214\n",
            "loss : 630.256463961462\n",
            "loss : 738.7919162851604\n",
            "loss : 601.8279057485381\n",
            "loss : 685.5834910747875\n",
            "loss : 717.8452906459025\n",
            "loss : 743.1095900591644\n",
            "loss : 767.281915871183\n",
            "loss : 642.2499842699469\n",
            "loss : 646.367546628624\n",
            "loss : 653.2455486217707\n",
            "loss : 558.0129821403693\n",
            "loss : 578.9609193267436\n",
            "loss : 505.57972096347146\n",
            "loss : 609.8798694377886\n",
            "loss : 693.1742187391408\n",
            "loss : 700.7050217894663\n",
            "loss : 761.474654008182\n",
            "loss : 736.3994631872342\n",
            "loss : 557.6315600716048\n",
            "loss : 755.1875000741998\n",
            "loss : 680.8025748608313\n",
            "loss : 700.1214802923203\n",
            "loss : 704.6336010122524\n",
            "loss : 742.4440120254445\n",
            "loss : 623.7938254287443\n",
            "loss : 672.8754913735947\n",
            "loss : 821.5379539985993\n",
            "loss : 735.1813028377085\n",
            "loss : 738.8353358982499\n",
            "loss : 732.9341989422308\n",
            "loss : 726.5712072257261\n",
            "loss : 651.2129906457869\n",
            "loss : 703.8135456620182\n",
            "loss : 650.284295792456\n",
            "loss : 723.5557399481145\n",
            "loss : 721.5843401632601\n",
            "loss : 718.2338278974164\n",
            "loss : 672.231565398172\n",
            "loss : 639.0990154266876\n",
            "loss : 700.9809758901047\n",
            "loss : 568.6342510880266\n",
            "loss : 685.428176883338\n",
            "loss : 729.8664810115789\n",
            "loss : 583.8187575995137\n",
            "loss : 577.6062688815152\n",
            "loss : 525.7145545552247\n",
            "loss : 641.7750365407751\n",
            "loss : 645.9034745104359\n",
            "loss : 609.8072786106416\n",
            "loss : 577.6327316564934\n",
            "loss : 465.51155441366336\n",
            "loss : 611.8110032885954\n",
            "loss : 563.0077552040011\n",
            "loss : 517.9814385081929\n",
            "loss : 668.4479132001752\n",
            "loss : 542.5195295116882\n",
            "loss : 582.6391854328891\n",
            "loss : 558.8252063166655\n",
            "loss : 613.682773512644\n",
            "loss : 501.470084484822\n",
            "loss : 587.4557920925097\n",
            "loss : 533.3908106712066\n",
            "loss : 503.87262264063713\n",
            "loss : 679.9027504898895\n",
            "loss : 567.9126895863515\n",
            "loss : 592.1288432704437\n",
            "loss : 704.978438400773\n",
            "loss : 615.874129391399\n",
            "loss : 688.1108243717088\n",
            "loss : 737.2509598082241\n",
            "loss : 529.6739374147968\n",
            "loss : 604.8006662903625\n",
            "loss : 608.2687503303379\n",
            "loss : 649.6299462485151\n",
            "loss : 712.0871700571063\n",
            "loss : 510.42198801499643\n",
            "loss : 535.1729145559434\n",
            "loss : 649.6374629518748\n",
            "loss : 662.425147228487\n",
            "loss : 528.311698185291\n",
            "loss : 633.2408218372307\n",
            "loss : 736.0968324853542\n",
            "loss : 708.8855101122779\n",
            "loss : 813.4232890547343\n",
            "loss : 673.0167177124076\n",
            "loss : 525.0533392198513\n",
            "loss : 664.836077197808\n",
            "loss : 627.5927208328396\n",
            "loss : 574.3942043141154\n",
            "loss : 434.8283590302535\n",
            "loss : 625.5158529814773\n",
            "loss : 572.8284988917654\n",
            "loss : 554.172041209818\n",
            "loss : 669.6185530307745\n",
            "loss : 559.6826925191956\n",
            "loss : 538.9334395061078\n",
            "loss : 618.0980559460493\n",
            "loss : 724.9604485846813\n",
            "loss : 569.6978309870817\n",
            "loss : 550.1314158523396\n",
            "loss : 596.7311743545401\n",
            "loss : 544.5109118633269\n",
            "loss : 542.9217905810244\n",
            "loss : 568.2637034775308\n",
            "loss : 557.7996602953272\n",
            "loss : 539.0705744977738\n",
            "loss : 483.8406614414323\n",
            "loss : 468.95327969854503\n",
            "loss : 648.9961711069096\n",
            "loss : 665.8603743555832\n",
            "loss : 648.1680721664951\n",
            "loss : 670.7361921184779\n",
            "loss : 540.5457602994634\n",
            "loss : 652.359395497948\n",
            "loss : 636.2672039817569\n",
            "loss : 599.423324164914\n",
            "loss : 507.7958779552673\n",
            "loss : 475.0863420323744\n",
            "loss : 456.78458157882153\n",
            "loss : 635.3112156301349\n",
            "loss : 529.6683481016305\n",
            "loss : 536.7394017350457\n",
            "loss : 621.8881298167619\n",
            "loss : 538.5673708986003\n",
            "loss : 507.35307577905076\n",
            "loss : 551.517721536286\n",
            "loss : 464.57990796293564\n",
            "loss : 604.4392076790466\n",
            "loss : 616.7128615949212\n",
            "loss : 682.91595984183\n",
            "loss : 473.47395606681727\n",
            "loss : 594.4714404726968\n",
            "loss : 675.6018422572267\n",
            "loss : 555.4145791883695\n",
            "loss : 617.5622565594069\n",
            "loss : 634.7927358936028\n",
            "loss : 606.5901134501562\n",
            "loss : 705.5509507877139\n",
            "loss : 610.9295515488443\n",
            "loss : 639.8405792028205\n",
            "loss : 498.9636203109647\n",
            "loss : 610.9325448247001\n",
            "loss : 526.8042476319496\n",
            "loss : 611.8897567651629\n",
            "loss : 595.2462424126572\n",
            "loss : 617.0211158604501\n",
            "loss : 637.4229540670686\n",
            "loss : 613.7211393477858\n",
            "loss : 616.928270943933\n",
            "loss : 611.2042955046659\n",
            "loss : 604.1473971951523\n",
            "loss : 660.8290879670848\n",
            "loss : 565.7673384786685\n",
            "loss : 660.1091145129348\n",
            "loss : 511.9487341255892\n",
            "loss : 643.688287015809\n",
            "loss : 489.5375627821842\n",
            "loss : 605.011693781888\n",
            "loss : 612.8459319792593\n",
            "loss : 647.6783841588253\n",
            "loss : 504.6042511645012\n",
            "loss : 705.6892824474826\n",
            "loss : 663.7909678389562\n",
            "loss : 555.1845466873813\n",
            "loss : 700.4561002874171\n",
            "loss : 575.2741523721334\n",
            "loss : 689.6266889793992\n",
            "loss : 635.4179482082503\n",
            "loss : 629.6747484705473\n",
            "loss : 597.059130414594\n",
            "loss : 614.4174505522351\n",
            "loss : 649.2715905755199\n",
            "loss : 604.8432939599936\n",
            "loss : 616.570000153145\n",
            "loss : 490.4901729916762\n",
            "loss : 521.3830774518045\n",
            "loss : 519.1925261886892\n",
            "loss : 706.0961605583132\n",
            "loss : 636.1696147512371\n",
            "loss : 656.0877092981946\n",
            "loss : 694.7250425448923\n",
            "loss : 604.2553164758444\n",
            "loss : 601.7766771174527\n",
            "loss : 643.7529673494028\n",
            "loss : 607.2916155857577\n",
            "loss : 640.7887634572794\n",
            "loss : 609.3328980072256\n",
            "loss : 764.0586404967172\n",
            "loss : 718.2861957144277\n",
            "loss : 718.132479978279\n",
            "loss : 669.5472263593284\n",
            "loss : 507.8901680926008\n",
            "loss : 673.9850533190424\n",
            "loss : 627.4607570794453\n",
            "loss : 583.0580870096385\n",
            "loss : 683.9890079580694\n",
            "loss : 623.8526150866563\n",
            "loss : 563.4440571379181\n",
            "loss : 616.0530416569968\n",
            "loss : 624.9326605686242\n",
            "loss : 607.0356307212984\n",
            "loss : 641.113812504429\n",
            "loss : 682.4951415243548\n",
            "loss : 595.9444801154966\n",
            "loss : 643.5343341073885\n",
            "loss : 494.8181981934581\n",
            "loss : 526.4022662899291\n",
            "loss : 634.6440082231888\n",
            "loss : 623.6525462673917\n",
            "loss : 611.6315122609669\n",
            "loss : 698.1413038108319\n",
            "loss : 548.7048268912944\n",
            "loss : 628.794825683968\n",
            "loss : 607.6715516575625\n",
            "loss : 637.9476128180726\n",
            "loss : 615.6175670252662\n",
            "loss : 649.2717680975966\n",
            "loss : 563.1366575537238\n",
            "loss : 462.386161063263\n",
            "loss : 693.7811618954236\n",
            "loss : 738.2796168385892\n",
            "loss : 712.5313352636451\n",
            "loss : 671.7128540674224\n",
            "loss : 556.1410270525143\n",
            "loss : 593.9123771292001\n",
            "loss : 624.1781994231931\n",
            "loss : 635.546827910188\n",
            "loss : 576.0435182271593\n",
            "loss : 526.1635557183781\n",
            "loss : 579.1771306314854\n",
            "loss : 509.4235234089043\n",
            "loss : 602.586848197815\n",
            "loss : 553.0098557598703\n",
            "loss : 629.2963157406072\n",
            "loss : 635.4728512466406\n",
            "loss : 614.727334275769\n",
            "loss : 592.6246861920449\n",
            "loss : 558.9768637301453\n",
            "loss : 517.4800779408131\n",
            "loss : 504.4801177050495\n",
            "loss : 458.17048142055336\n",
            "loss : 582.3274264582861\n",
            "loss : 500.4015099484697\n",
            "loss : 569.4210608769258\n",
            "loss : 621.3577340859609\n",
            "loss : 497.9560230294439\n",
            "loss : 521.8131712464783\n",
            "loss : 570.4714819247729\n",
            "loss : 508.01065359156684\n",
            "loss : 509.785497917519\n",
            "loss : 570.5726700090477\n",
            "loss : 537.9624210540927\n",
            "loss : 540.0441369050204\n",
            "loss : 535.3570136682612\n",
            "loss : 739.0750545487587\n",
            "loss : 599.063612864414\n",
            "loss : 569.3359356881472\n",
            "loss : 482.05096012483307\n",
            "loss : 531.9792398537909\n",
            "loss : 649.0443133375327\n",
            "loss : 566.7374495408808\n",
            "loss : 607.8625728939032\n",
            "loss : 556.3379261511766\n",
            "loss : 572.2450235797128\n",
            "loss : 487.00174492538565\n",
            "loss : 571.102776014216\n",
            "loss : 546.19590528921\n",
            "loss : 618.2379191377436\n",
            "loss : 678.9187039392493\n",
            "loss : 491.60099688707567\n",
            "loss : 569.7848312272984\n",
            "loss : 480.17066871203815\n",
            "loss : 600.4407112310892\n",
            "loss : 592.6901523522357\n",
            "loss : 467.80893163780183\n",
            "loss : 536.688287380851\n",
            "loss : 516.6244867657991\n",
            "loss : 502.10554702573137\n",
            "loss : 456.12402900151903\n",
            "loss : 550.129117686384\n",
            "loss : 499.486878607001\n",
            "loss : 500.6568941703967\n",
            "loss : 453.2915546004636\n",
            "loss : 562.2345401055902\n",
            "loss : 582.9119717880617\n",
            "loss : 607.4010141596564\n",
            "loss : 632.6777559773275\n",
            "loss : 504.2406192845276\n",
            "loss : 469.6084069350621\n",
            "loss : 570.0208538751601\n",
            "loss : 597.2996879995392\n",
            "loss : 451.56303675183005\n",
            "loss : 466.0881507013434\n",
            "loss : 587.8122318868052\n",
            "loss : 615.1913327064915\n",
            "loss : 607.8869968935041\n",
            "loss : 624.3408748087204\n",
            "loss : 702.4755973053702\n",
            "loss : 535.316405791022\n",
            "loss : 590.2756267728146\n",
            "loss : 566.5717687742851\n",
            "loss : 620.2259714349234\n",
            "loss : 567.1206715877438\n",
            "loss : 515.4153837718354\n",
            "loss : 571.2233807892796\n",
            "loss : 582.0104427807762\n",
            "loss : 723.4776810360622\n",
            "loss : 641.014844234737\n",
            "loss : 646.4320931791653\n",
            "loss : 652.0131591408266\n",
            "loss : 577.6377288768817\n",
            "loss : 608.146962498278\n",
            "loss : 591.9172843469591\n",
            "loss : 532.2584433560863\n",
            "loss : 576.6630539844107\n",
            "loss : 433.1592780093417\n",
            "loss : 601.3845384061005\n",
            "loss : 514.0693442010916\n",
            "loss : 506.0419688363281\n",
            "loss : 460.09767446197026\n",
            "loss : 590.0878717534077\n",
            "loss : 517.556358425441\n",
            "loss : 618.0415181735209\n",
            "loss : 615.4181991988343\n",
            "loss : 740.9880555071816\n",
            "loss : 543.5034425967916\n",
            "loss : 565.1494931216014\n",
            "loss : 544.8538597154486\n",
            "loss : 641.6237115022533\n",
            "loss : 691.9904362760813\n",
            "loss : 515.9402347440895\n",
            "loss : 564.3767367145778\n",
            "loss : 536.8957392230233\n",
            "loss : 545.8390636368506\n",
            "loss : 510.59746682255087\n",
            "loss : 591.5021949684135\n",
            "loss : 662.4416690700095\n",
            "loss : 554.3902033589707\n",
            "loss : 593.6640422471804\n",
            "loss : 479.10727586546903\n",
            "loss : 490.1003046031063\n",
            "loss : 505.74338173976366\n",
            "loss : 617.5233457962988\n",
            "loss : 672.2700414318642\n",
            "loss : 556.0381413020062\n",
            "loss : 616.0943337879587\n",
            "loss : 586.2178379637598\n",
            "loss : 696.6226620383377\n",
            "loss : 573.453817342425\n",
            "loss : 442.4986761178694\n",
            "loss : 455.91643209683286\n",
            "loss : 501.38470349289213\n",
            "loss : 552.8174642602326\n",
            "loss : 521.096373671375\n",
            "loss : 578.7166503325689\n",
            "loss : 421.3367594538877\n",
            "loss : 687.2003110622811\n",
            "loss : 487.96823716258666\n",
            "loss : 468.7073982320408\n",
            "loss : 591.9197682642844\n",
            "loss : 494.39580276558166\n",
            "loss : 449.15594869199\n",
            "loss : 528.6783949584762\n",
            "loss : 511.59142655307596\n",
            "loss : 593.6242628627056\n",
            "loss : 683.4145756188695\n",
            "loss : 608.471975520917\n",
            "loss : 537.8814574962425\n",
            "loss : 523.3208405559319\n",
            "loss : 640.6250797712607\n",
            "loss : 752.0467580408223\n",
            "loss : 447.2215355248981\n",
            "loss : 534.7705758520412\n",
            "loss : 597.5360945798146\n",
            "loss : 565.124166460665\n",
            "loss : 532.8421906787072\n",
            "loss : 495.0600032007281\n",
            "loss : 670.2208434379548\n",
            "loss : 660.3097720474958\n",
            "loss : 591.8780448745912\n",
            "loss : 505.5758878294039\n",
            "loss : 552.7710000550692\n",
            "loss : 557.9287935327311\n",
            "loss : 545.659729806296\n",
            "loss : 524.630667828335\n",
            "loss : 488.101941635731\n",
            "loss : 562.011739636496\n",
            "loss : 567.2592484211921\n",
            "loss : 531.7391801332286\n",
            "loss : 534.0424124405067\n",
            "loss : 532.3284634491065\n",
            "loss : 625.8631231598008\n",
            "loss : 536.7888944046056\n",
            "loss : 531.4266304311379\n",
            "loss : 610.0951851152981\n",
            "loss : 429.5682365201951\n",
            "loss : 552.315433110897\n",
            "loss : 542.4161433191866\n",
            "loss : 466.6157954664252\n",
            "loss : 518.0943994274405\n",
            "loss : 531.3035999868758\n",
            "loss : 564.3443586418583\n",
            "loss : 493.72190931345324\n",
            "loss : 551.7752779425718\n",
            "loss : 516.8576540716994\n",
            "loss : 574.9974730687782\n",
            "loss : 618.8171811113968\n",
            "loss : 572.1917582857291\n",
            "loss : 512.9619645948137\n",
            "loss : 551.6898465007552\n",
            "loss : 572.0704143612123\n",
            "loss : 544.4079627424193\n",
            "loss : 506.398291978219\n",
            "loss : 489.8800904501335\n",
            "loss : 542.8243381904531\n",
            "loss : 534.2107773416387\n",
            "loss : 552.1997759905071\n",
            "loss : 597.2506223742968\n",
            "loss : 452.2464831636609\n",
            "loss : 588.142967004801\n",
            "loss : 629.7431767085154\n",
            "loss : 620.9347584939967\n",
            "loss : 530.7931697122224\n",
            "loss : 598.6369984425021\n",
            "loss : 574.1510607175626\n",
            "loss : 431.3226326520095\n",
            "loss : 550.1069444280807\n",
            "loss : 654.2194831780628\n",
            "loss : 542.4962774019347\n",
            "loss : 543.1238616117932\n",
            "loss : 571.7700959309873\n",
            "loss : 452.951079776562\n",
            "loss : 423.71381788281747\n",
            "0.913301282051282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(acc(model, np.reshape(vX, (vX.shape[0], -1)) / 255, vY ))"
      ],
      "metadata": {
        "id": "b115hRZFHmK2",
        "outputId": "c53216f5-1efa-42f6-b4ee-dfbc0baeff18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8435496794871795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ".0 * np.exp(1000)"
      ],
      "metadata": {
        "id": "Hx5Q1j6DFFYM",
        "outputId": "7ee54052-18f9-44b5-87c7-3e8fc04c6bab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-268-9da1953d9d6a>:1: RuntimeWarning: overflow encountered in exp\n",
            "  .0 * np.exp(1000)\n",
            "<ipython-input-268-9da1953d9d6a>:1: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  .0 * np.exp(1000)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yPred =  model.predict(x / 255)\n",
        "yPred = np.argmax(yPred, 1)\n",
        "accuracy_score(yTrue, yPred)"
      ],
      "metadata": {
        "id": "dn_dnSv121zL",
        "outputId": "473d32ef-56d9-4b72-f444-aa160a039a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.906803534523321\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-253-2714a610ed36>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0myPred\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0myPred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myPred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myPred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [49920, 1000]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5SBl7zVY3LPf",
        "outputId": "c8e4788d-62f4-4892-d72f-14f821cfebd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.239103122634079\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.029467147435897435"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "xs = x[0:n]\n",
        "ys = y[0:n]"
      ],
      "metadata": {
        "id": "VwwHnx0N1RI4"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(xs / 255.,ys, GradientDescent(.0001), 1, 256)"
      ],
      "metadata": {
        "id": "Onq0opHP0prZ",
        "outputId": "d07e33f4-34a5-4fee-c951-0641c189b457",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "loss : nan\n",
            "nan\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "loss : nan\n",
            "nan\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "loss : nan\n",
            "nan\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "loss : nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-241-fa3eaf1002d4>:195: RuntimeWarning: divide by zero encountered in log\n",
            "  return -np.mean(yTrue * np.log(yPred))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(x[0] / 255.)"
      ],
      "metadata": {
        "id": "esOStDJLu8mo",
        "outputId": "cf0a7096-cac3-4a96-8942-2a90cf75d171",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15.441225793984904\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.64196872e-04, 4.36235994e-09, 8.61792079e-04, ...,\n",
              "        2.20580202e-08, 3.11272512e-02, 5.31570266e-03],\n",
              "       [2.56509971e-02, 6.85556669e-04, 2.63567954e-02, ...,\n",
              "        1.00783256e-03, 8.29320561e-02, 5.45343356e-02],\n",
              "       [1.58918968e-02, 6.54861973e-05, 1.58530629e-02, ...,\n",
              "        1.27993558e-04, 8.60837071e-02, 4.13998095e-02],\n",
              "       ...,\n",
              "       [4.34255141e-02, 3.71269625e-02, 2.68246024e-02, ...,\n",
              "        4.00804187e-02, 3.71267259e-02, 4.98456457e-02],\n",
              "       [2.67300361e-05, 1.06345797e-13, 2.13174387e-05, ...,\n",
              "        1.43278060e-12, 5.44088365e-03, 3.19783847e-04],\n",
              "       [3.67652439e-02, 2.94144052e-02, 3.19929873e-02, ...,\n",
              "        2.92797833e-02, 4.70056717e-02, 4.94279541e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xy = list(zip(x, y))\n",
        "np.random.shuffle(xy)\n",
        "\n",
        "# Separate shuffled images and labels\n",
        "x, y = zip(*xy)\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "batchSize = 64\n",
        "numExa = x.shape[0]\n",
        "numBatches = numExa // batchSize\n",
        "remSamples = numExa % batchSize\n",
        "\n",
        "\n",
        "xB = np.array_split(x[:numBatches * batchSize], numBatches)\n",
        "yB = np.array_split(y[:numBatches * batchSize], numBatches)\n",
        "\n",
        "\n",
        "if remSamples > 0:\n",
        "  xB.append(x[-remSamples:])\n",
        "  yB.append(y[-remSamples:])\n",
        "  numBatches += 1\n"
      ],
      "metadata": {
        "id": "MmYoRrOf3oG3"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xB[0].shape"
      ],
      "metadata": {
        "id": "4BYr8VWt_9Ol",
        "outputId": "6764275e-6483-4d25-b4a8-91961f2e2e6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomPlot(np.reshape(xB[0], (batchSize, 28, 28)), yB[0])"
      ],
      "metadata": {
        "id": "WDqbUrKn_e9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OGDsVhKG7iVd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}