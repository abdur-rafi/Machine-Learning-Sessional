{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Input:\n",
    "\t# input shape: (batch_size, #features)\n",
    "\t# output shape: (batch_size, #features)\n",
    "\n",
    "\tdef __init__(self, inputFeatures) -> None:\n",
    "\t\tself.inputShape = (-1, inputFeatures)\n",
    "\t\tself.outputShape = self.inputShape\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x\n",
    "\t\n",
    "\tdef backward(self, gradientLossWRTOutput, _):\n",
    "\t\treturn gradientLossWRTOutput, None\n",
    "\t\n",
    "\tdef getWeights(self):\n",
    "\t\treturn None\n",
    "\t\n",
    "\n",
    "\n",
    "class Dense:\n",
    "\t# input shape: (batch_size, #features)\n",
    "\t# output shape: (batch_size, #nodes)\n",
    "\n",
    "\tdef __init__(self, numNodes) -> None:\n",
    "\t\tself.numNodes = numNodes\n",
    "\t\n",
    "\t# input shape: (batch_size, #features)\n",
    "\tdef initPipeline(self, inputShape):\n",
    "\t\tinputFeatures = inputShape[1]\n",
    "\t\tself.features = inputFeatures\n",
    "\n",
    "\t\tself.weights = np.random.randn(self.numNodes, inputFeatures)\n",
    "\t\t# self.weights = np.ones((self.numNodes, inputFeatures))\n",
    "\t\t# print(self.weights.shape)\n",
    "\t\tself.bias = np.random.randn(self.numNodes, 1)\n",
    "\t\t# self.bias = np.ones((self.numNodes, 1))\n",
    "\t\t# print(self.bias.shape)\n",
    "\t\tself.outputShape = (-1, self.numNodes)\n",
    "\n",
    "\t# x shape: (batch_size, #features)\n",
    "\tdef forward(self, x):\n",
    "\t\tself.x = x\n",
    "\t\tself.y = np.dot(self.weights, x.T) + self.bias\n",
    "\t\tself.y = self.y.T\n",
    "\t\treturn self.y\n",
    "\t\n",
    "\t# gradientLossWRTOutput shape: (batch_size, #nodes)\n",
    "\n",
    "\tdef backward(self, gradientLossWRTOutput,optimizer):\n",
    "\t\t\n",
    "\t\tgradientLossWRTInput = np.dot(gradientLossWRTOutput, self.weights)\n",
    "\n",
    "\t\t\n",
    "\t\tgradientLossWRTWeights = np.dot(gradientLossWRTOutput.T, self.x)\n",
    "\t\tgradientLossWRTBias = np.sum(gradientLossWRTOutput, axis = 0, keepdims=True).T\n",
    "\n",
    "\t\t# print(f\"gradientLossWRTWeights : {gradientLossWRTWeights.shape}\")\n",
    "\t\t# print(f\"gradientLossWRTbias : {gradientLossWRTBias.shape}\")\n",
    "\n",
    "\n",
    "\t\tself.weights = optimizer.update(self.weights, gradientLossWRTWeights)\n",
    "\t\tself.bias = optimizer.update(self.bias, gradientLossWRTBias)\n",
    "\n",
    "\t\t# self.weights -= learningRate * gradientLossWRTWeights\n",
    "\t\t# self.bias -= learningRate * gradientLossWRTBias\n",
    "\n",
    "\t\treturn gradientLossWRTInput, (gradientLossWRTWeights, gradientLossWRTBias)\n",
    "\t\n",
    "\tdef getWeights(self):\n",
    "\t\treturn (self.weights, self.bias)\n",
    "\t\t\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tpass\n",
    "\n",
    "\t# input shape: (batch_size, #features)\n",
    "\t# output shape: (batch_size, #features)\n",
    "\n",
    "\tdef initPipeline(self, inputShape):\n",
    "\t\tself.inputShape = inputShape\n",
    "\t\tself.outputShape = inputShape\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tself.x = x\n",
    "\t\tself.y =  np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\t\treturn self.y\n",
    "\t\n",
    "\t# gradientLossWRTOutput shape: (batch_size, #features)\n",
    "\n",
    "\tdef backward(self, gradientLossWRTOutput, _):\n",
    "\t\t# gradientOutputWRT\n",
    "\n",
    "\t\tn , m = self.y.shape\n",
    "\n",
    "\t\tgradientOutputWRTInput = np.repeat(self.y, m, axis=0).reshape(n, m, m)\n",
    "\t\tgradientOutputWRTInput = np.multiply(gradientOutputWRTInput, np.transpose(gradientOutputWRTInput, axes=(0, 2, 1))) * -1\n",
    "\n",
    "\t\tdiagElems = np.reshape(self.y, (n,  m , 1))\n",
    "\t\tdiagElems = diagElems * (1 - diagElems)\n",
    "\t\tdiagElems = np.eye(m) * diagElems\n",
    "\t\tmask = np.eye(m, dtype=bool)\n",
    "\t\tmask = np.tile(mask, (n, 1)).reshape(n, m, m)\n",
    "\t\tgradientOutputWRTInput[mask] = 0\n",
    "\t\tgradientOutputWRTInput = gradientOutputWRTInput + diagElems\n",
    "\n",
    "\t\t# jacobian_matrix = np.zeros((n, m, m))\n",
    "\n",
    "\t\t# for i in range(n):\n",
    "\t\t# \t# s = np.exp(self.x[i]) / np.sum(np.exp(self.x[i]), axis=1, keepdims=True)\n",
    "\t\t# \ts = self.y[i]\n",
    "\t\t# \tfor j in range(n):\n",
    "\t\t# \t\tfor k in range(n):\n",
    "\t\t# \t\t\tjacobian_matrix[i, j, k] = s[j] * (int(j == k) - s[k])\n",
    "\n",
    "\n",
    "\t\t# print(np.isclose(jacobian_matrix, gradientOutputWRTInput).all())\n",
    "\t\t# print(f\"gradientLossWRTOutput : {np.expand_dims(gradientLossWRTOutput, -1).shape}\")\n",
    "\n",
    "\n",
    "\t\tgradientLossWRTInput = np.matmul(gradientOutputWRTInput, np.expand_dims(gradientLossWRTOutput, -1))\n",
    "\t\tgradientLossWRTInput = np.squeeze(gradientLossWRTInput)\n",
    "\n",
    "\t\t# print(f\"gradientLossWRTInput : {gradientLossWRTInput.shape}\")\n",
    "\n",
    "\t\treturn gradientLossWRTInput, None\n",
    "\t\n",
    "\tdef getWeights(self):\n",
    "\t\treturn None\n",
    "\t\n",
    "\n",
    "class Relu:\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tpass\n",
    "\n",
    "\tdef initPipeline(self, inputShape):\n",
    "\t\tself.inputShape = inputShape\n",
    "\t\tself.outputShape = inputShape\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tself.x = x\n",
    "\t\tself.y = np.maximum(x, 0)\n",
    "\t\treturn self.y\n",
    "\n",
    "\t# gradientLossWRTOutput shape: (batch_size, #features)\n",
    "\tdef backward(self, gradientLossWRTOutput, _):\n",
    "\t\tgradientOutputWRTInput = np.where(self.x > 0, 1, 0)\n",
    "\t\tgradientLossWRTInput = np.multiply(gradientOutputWRTInput, gradientLossWRTOutput)\n",
    "\t\treturn gradientLossWRTInput, None\n",
    "\t\n",
    "\tdef getWeights(self):\n",
    "\t\treturn None\n",
    "\t\n",
    "\t\n",
    "\n",
    "class Model:\n",
    "\tdef __init__(self, *layers) -> None:\n",
    "\t\tself.nLayers = len(layers)\n",
    "\n",
    "\t\tfor i in range(1, self.nLayers):\n",
    "\t\t\tlayers[i].initPipeline(layers[i-1].outputShape)\n",
    "\t\t\n",
    "\t\tself.layers = layers\n",
    "\t\n",
    "\tdef predict(self, x):\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tx = layer.forward(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef crossEntropyGradient(self, yTrue, yPred):\n",
    "\t\treturn - yTrue / yPred\n",
    "\n",
    "\tdef __backprop(self,yTrue, yPred, optimizer):\n",
    "\t\t# gradientLossWRTOutput = self.crossEntropyGradient(yTrue, yPred)\n",
    "\t\tgradientLossWRTOutput = self.squaredErrorGradient(yTrue, yPred)\n",
    "\n",
    "\t\t# print(f\"loss grad wrt yhat : {gradientLossWRTOutput.shape}\")\n",
    "\t\t# wAndg = []\n",
    "\n",
    "\t\t\n",
    "\t\tfor layer in reversed(self.layers):\n",
    "\t\t\tgradientLossWRTOutput, g = layer.backward(gradientLossWRTOutput, optimizer)\n",
    "\t\t\t# wAndg.append((layer.getWeights(), g))\n",
    "\n",
    "\t\t# return wAndg\n",
    "\t\n",
    "\tdef train(self, x, y, optimizer, epoch):\n",
    "\t\tfor i in range(epoch):\n",
    "\t\t\tyPred = self.predict(x)\n",
    "\t\t\t# loss = self.__loss(y, yPred)\n",
    "\t\t\t# print(f\"epoch {i} : {loss}\")\n",
    "\t\t\tself.__backprop(y, yPred, optimizer)\n",
    "\n",
    "\tdef squaredErrorGradient(self, yTrue, yPred):\n",
    "\t\treturn -2 * (yTrue - yPred)\t\t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GradientDescent:\n",
    "\tdef __init__(self, learningRate) -> None:\n",
    "\t\tself.learningRate = learningRate\n",
    "\t\n",
    "\tdef update(self, w, g):\n",
    "\t\treturn w - self.learningRate * g\n",
    "\t\n",
    "\n",
    "def eqn(x):\n",
    "\treturn 2 * (x[0] ** 2) + 3.5 * x[1] + 7\n",
    "\n",
    "def main():\n",
    "\t\n",
    "\txs = []\n",
    "\tys = []\n",
    "\tfor i in range(100):\n",
    "\t\tx = np.random.randn(2)\n",
    "\t\txs.append(x)\n",
    "\t\tys.append(eqn(x))\n",
    "\tx = np.vstack(xs)\n",
    "\ty = np.vstack(ys)\n",
    "\n",
    "\tprint(x.shape)\n",
    "\tprint(y.shape)\n",
    "\n",
    "\n",
    "\t\n",
    "\tmodel = Model(\n",
    "\t\tInput(2),\n",
    "\t\tDense(4),\n",
    "\t\tRelu(),\n",
    "\t\tDense(3),\n",
    "\t\tRelu(),\n",
    "\t\tDense(1)\n",
    "\t\t# Dense(3),\n",
    "\t\t# Dense(2),\n",
    "\t\t# Softmax()\n",
    "\t\t# Softmax()\n",
    "\t)\n",
    "\t# print(model.predict(x))\n",
    "\t# x = np.array([[1, -3], [-1, 1], [5, 6]])\n",
    "\t# y = np.array([[0, 1, 0], [1, 0, 0], [0,0, 1]])\n",
    "\t# print(x.shape)\n",
    "\t# yPred = model.predict(x)\n",
    "\t# print(f\"ypred : {yPred}\")\n",
    "\t# print(f\"yPred.shape : {yPred.shape}\")\t\n",
    "\txt = np.array([[.5, .36]])\n",
    "\n",
    "\t# print(model.predict(xt))\n",
    "\t# print(eqn(xt[0]))\n",
    "\n",
    "\tprint(np.sum(np.square(model.predict(x) - y)) / 100)\n",
    "\n",
    "\tmodel.train(x, y, GradientDescent(0.0001), 1000)\n",
    "\n",
    "\tprint(np.sum(np.square(model.predict(x) - y)) / 100)\n",
    "\n",
    "\n",
    "\t# print(model.predict(x))\n",
    "\n",
    "\n",
    "\tprint(model.predict(xt))\n",
    "\tprint(eqn(xt[0]))\n",
    "\t# print(model.predict(x))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()\n",
    "\n",
    "\n",
    "# Your initial array of shape (n, m)\n",
    "# original_array = np.array([[1, 2, 3],\n",
    "# \t\t\t\t\t\t[4, 5, 6]])\n",
    "\n",
    "# n, m = original_array.shape\n",
    "\n",
    "# # Repeat each row m times\n",
    "# repeated_rows = np.repeat(original_array, m, axis=0)\n",
    "\n",
    "# # Reshape the repeated array to (n, m, m)\n",
    "# result_array = repeated_rows.reshape(n, m, m)\n",
    "\n",
    "# # print(result_array)\n",
    "\n",
    "# # print(np.reshape(original_array, (n,  m , 1)))\n",
    "\n",
    "# diagElems = np.reshape(original_array, (n,  m , 1))\n",
    "# # diagElems = original_array\n",
    "# diagElems = diagElems * (1 - diagElems)\n",
    "# diagElems = np.eye(m) * diagElems\n",
    "# print(diagElems)\n",
    "\n",
    "# mask = np.eye(m, dtype=bool)\n",
    "# mask = np.tile(mask, (n, 1)).reshape(n, m, m)\n",
    "\n",
    "# # result_array = np.fill_diagonal(result_array, diagElems)\n",
    "# # np.fill_diagonal(result_array, 0)\n",
    "# result_array[mask] = 0\n",
    "# result_array = result_array + diagElems\n",
    "\n",
    "# print(result_array)\n",
    "# # print(diagElems)\n",
    "\n",
    "# print(np.multiply(result_array, np.transpose(result_array, axes=(0, 2, 1))))\n",
    "\n",
    "\n",
    "# print(np.multiply(result_array, np.reshape(original_array, (n, m, 1))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
